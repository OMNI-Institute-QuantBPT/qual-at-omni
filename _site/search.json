[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Omni Quarto Website Template",
    "section": "",
    "text": "This is a Quarto website template, made for the Omni Institute.\nIt’s based on Omni branding and is intended to be easy to customize using Quarto.\nBy default, it contains 3 pages:\n\nHome: the page you’re currently in\nUse this template: a short tutorial on how to use this website template\nCustomization: a list of nice customizations you can make with Quarto, and how to do them"
  },
  {
    "objectID": "customization.html",
    "href": "customization.html",
    "title": "Customization",
    "section": "",
    "text": "Executive Summary"
  },
  {
    "objectID": "customization.html#purpose",
    "href": "customization.html#purpose",
    "title": "Customization",
    "section": "Purpose",
    "text": "Purpose\nThis guide was developed to support Omni staff in conducting methodologically sound, transparent, and reproducible qualitative analyses. It offers practical, step-by-step instructions and highlights best practices for text pre-processing, analysis, and reporting. Whether working with interview transcripts, focus groups, open-ended survey responses, or other text-based data, Omni teams can use this guide to produce findings that are both grounded in participants’ voices and useful for program improvement and decision-making.\n\nHow does this guide align with Omni’s core values?\n\n\n\n  \n    \n      Inquiry\n      \n        Provides clear, rigorous methods that encourage thoughtful exploration of participants’ experiences.\n      \n    \n  \n\n  \n    \n      Agility\n      \n        Equips staff with adaptable tools to meet diverse project needs and respond efficiently to change.\n      \n    \n  \n\n  \n    \n      Connection\n      \n        Centers participants’ voices and strengthens relationships with the communities and partners we serve.\n      \n    \n  \n\n  \n    \n      Accountability\n      \n        Uses transparent, reproducible analysis practices to keep our work credible, ethical, and trustworthy."
  },
  {
    "objectID": "customization.html#what-this-guide-covers",
    "href": "customization.html#what-this-guide-covers",
    "title": "Customization",
    "section": "What This Guide Covers",
    "text": "What This Guide Covers\nYou’ll find guidance on:\n\nText mining / Natural Language Processing (NLP)\n(e.g., word frequency, sentiment analysis, topic modeling)\nThematic and content analysis\n(e.g., dictionary-based coding, thematic coding frameworks)\nNarrative analysis @qualBPT do we need?\n(Optional: e.g., structural analysis, language style matching)\nVisualizing and reporting qualitative data\n(e.g., word clouds, bar charts, heatmaps, joint displays for mixed-methods)\n\nThese methods can be applied to text from: Word documents (.docx), PDFs (.pdf), Zoom transcripts (.docx or .txt), open-ended survey responses (.xlsx or .csv), and more.\nIn many applied settings, qualitative methods (and especially mixed-methods) are described in vague terms. Reports may mention “themes emerging” or “triangulating findings,” but rarely explain the actual process used to get there. This vagueness makes it hard to replicate or assess qualitative findings and limits their credibility.\n\n\n\n\n\n\nAt Omni, we aim to avoid vague or ad hoc practices. We document\nclear, systematic workflows that integrate qualitative insights with\nquantitative data when appropriate.\n\n\n\n\nWhy Are Existing Descriptions of Methods So Vague?\nSeveral factors explain the common lack of clarity in methodology:\n\nDifferent disciplines (e.g., public health vs. education research) use different frameworks, leading to inconsistent approaches.\nQuantitative research has standard tools (R, SPSS, Stata), but qualitative tools (NVivo, MAXQDA, Dedoose) often rely on manual processes and don’t always integrate cleanly with quantitative workflows.\nQualitative analysis requires human interpretation and reflexivity. Documenting every step is time-consuming, and under tight timelines, many organizations skip this critical process.\nQualitative and quantitative teams often work separately, without shared standards or mixed-methods integration.\n\n[Hannah is just messing around here to see how an .mp4 reads into R and plays]\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n  \n    Omni's Approach to Analysis\n    \n      \n    \n  \n\n\nOmni’s qualitative practice is grounded in pragmatism and critical realism.\n\nPragmatism means we focus on providing useful, actionable insights for real-world decision-making. Our goal is to help clients understand participants’ experiences in ways that inform policy, programs, and practice. This reflects a pragmatic stance rooted in the work of scholars like  Morgan (2007)  and  Danermark et al. (2015) , emphasizing inquiry that is guided by consequences and utility.\nCritical realism acknowledges that while participants’ experiences reflect real phenomena (barriers, challenges, successes), they are shaped by context and perspective. Influenced by  Fletcher’s (2017)  work and work by others, this perspective guides us to acknowledge our limitations, reflect on researcher influence, and avoid overstating claims.\nOmni’s qualitative practice draws on reflexive thematic analysis  (Clarke & Braun, 2006)  and content analysis  (Hseih & Shannon, 2005) , emphasizing the researcher’s active role in interpreting data and constructing themes, in alignment with our commitment to critical realism and methodological transparency.\n\n\n\nTransparent and Systematic Workflows\nAt Omni, we use structured qualitative and mixed-methods workflows. These processes are transparent, systematic, and designed to integrate qualitative and quantitative findings when appropriate.\n\nExample Omni Workflow for Qualitative analysis:\n\nClarify Purpose and Analytic Framework\n\n\nDefine the evaluation questions and purpose of the qualitative analysis\nSelect an analytic method\n\n\nPrepare and Familiarize with the Data\n\n\nClean transcripts/text\nRead through interviews, discussions, or sessions at least once (text like reports you may not need context for, use your discretion)\n\n\nSystematic Coding of the Data\n\n\nDeductive, Inductive, or hybrid\nDeductive coding uses predefined codes (like our dictionaries) to apply to our data\nInductive coding uses text mining or topic modeling to identify emergent themes\n\n\nTheme Development and Refinement\n\n\nAnalyze and refine codes/themes\n\n\nInterpretation and Synthesis\n\n\nCompare themes to evaluation questions and contextual frameworks\n\n\nExplore themes visually\nGet ready for reporting"
  },
  {
    "objectID": "customization.html#choosing-your-analysis-tool-r-vs.-dedoose-vs.-ai",
    "href": "customization.html#choosing-your-analysis-tool-r-vs.-dedoose-vs.-ai",
    "title": "Customization",
    "section": "Choosing Your Analysis Tool: R vs. Dedoose vs. AI",
    "text": "Choosing Your Analysis Tool: R vs. Dedoose vs. AI\n\n\n\n  \n\n    \n    \n      R\n      \n        \n          Best when\n          \n            Datasets are large or mixed (qual + quant), need audit trails.\n            You want reproducibility, automation, version control.\n            Time crunch with existing scripts/templates; IRR optional or phased.\n          \n        \n        \n          Pros\n          \n            Transparent, reproducible code; scalable pipelines.\n            Flexible: word freq, sentiment, dictionaries, topic models.\n            Precise visuals; integrates surveys/demographics easily.\n          \n          Cons\n          \n            Setup/learning curve.\n            Requires clear documentation to ensure shared understanding.\n          \n        \n      \n    \n\n    \n    \n      Dedoose\n      \n        \n          Best when\n          \n            Collaborative coding with a team; training new coders.\n            Small–moderate corpora.\n            Quick turnarounds with IRR workflows built in the tool.\n          \n        \n        \n          Pros\n          \n            Multi‑user collaboration, role permissions, IRR features.\n            Low barrier for non‑programmers.\n          \n          Cons\n          \n            License cost.\n            Automation/custom analyses limited vs. code.\n            Reproducibility/export trails less granular than scripted pipelines.\n          \n        \n      \n    \n\n    \n    \n      AI\n      \n        \n          Best when\n          \n            Early exploration: surfacing possible themes/codes quickly.\n            Short corpora; scoping questions; drafting a codebook.\n            Stakeholder previews before deeper human analysis.\n          \n        \n        \n          Pros\n          \n            Very fast; low barrier; helps brainstorm structures.\n            Can summarize, cluster, and compare at a glance.\n            Useful for iteration before moving to R or Dedoose.\n          \n          Cons\n          \n            Not a substitute for human interpretation; can hallucinate.\n            Inconsistent across runs; shallow context if prompts are weak.\n            Data governance/privacy limits—avoid sensitive uploads without approvals.\n          \n        \n      \n    \n\n  \n\n  \n  ‹\n  ›\n\n  \n  \n    \n    \n    \n  \n\n\n\n\n\n\n  \n    Method Selection\n    \n      \n    \n  \n\n\n\n\nOnce pre-processed, your data is ready for analysis. Selecting the right method depends on:\n\nHow much data you have\nYour research question(s)\nWhether you need exploratory insights or answers to specific questions\nYour integrative framework (if conducting mixed-methods)\n\n\n\n\n\n\nUnderstanding Your Data Before Analysis\nBefore starting pre-processing or analysis, it’s important to assess the scope and quality of your data. Take time to understand:\n\nHow many participants are included?\nHow much text do you have (word count, number of responses)?\nHow detailed are the responses (in-depth vs. brief)?\nHow are participants grouped (by stakeholder type, session, etc.)?\nWhose voices are most important to elevate in this analysis?\n\nThis step ensures you choose methods that fit your dataset and align with your project goals. For example:\n\nSmaller datasets (under ~2,000 words) are well-suited to word frequency, sentiment analysis, or basic content analysis.\nLarger datasets (5,000+ words) can support topic modeling or advanced thematic analysis.\n\nAssessing your data upfront helps set realistic expectations for analysis and ensures your findings are grounded, transparent, and defensible.\n\n\nWorking with a Small Corpus\nAt Omni, qualitative research often happens in real-world settings, with tight timelines and limited participant availability. We may aim for 10 interviews and complete 5. We may expect long, detailed conversations and instead get brief answers. Even with these constraints, small datasets can still provide meaningful insights, especially when there is consistency across participant experiences.\nWhen working with limited data:\n\nFocus on what participants actually shared, rather than attempting to generalize.\nDocument patterns and recurring concerns, while linking them clearly to the project’s research questions.\nBe transparent about the number of participants, the methods used, and any limitations in interpretation.\n\nSmaller samples can still highlight critical issues—such as barriers to access or common recommendations for program improvement—but the scope and representativeness of these findings should be clearly communicated.\n\n\nSuggested Methods by Dataset Size and Purpose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: If you are working with open-ended\nresponses from a survey and have data from 30 or fewer\nparticipants, carefully consider whether the data are\nsufficient for meaningful analysis. In many cases, it is appropriate to\nreport that there were too few responses to analyze\nsystematically. However, if you need to report findings, be\ntransparent about limitations. Focus on describing frequently\nmentioned key words, rather than inferring broader themes.\nAvoid suggesting consensus when data are limited, and clearly state that\nfindings reflect input from a small number of\nparticipants. Transparent reporting maintains the credibility\nof your analysis.\n\n\n\n\n\nGrouping Voices to Guide Analysis\nBefore you start coding or analyzing, decide:\n\nWhose voices are we centering in this analysis?\nHow will we group participant responses?\n\nYour grouping choices influence:\n\nWhich perspectives are highlighted\nHow themes emerge\nWhat questions you can answer\n\nAt Omni, we might group data by:\n\nParticipant (individual experiences)\nStakeholder Group (e.g., educators vs. public health professionals)\nDiscussion Section (e.g., barriers vs. recommendations)\n\nThese decisions should align with the project’s goals and be clearly documented in reports and presentations.\n\n\nReady for Pre-processing?\nOnce you understand your data, you’re ready to start pre-processing!\n\n\n  \n    Cleaning and Pre-Processing Your Data"
  },
  {
    "objectID": "customization.html#terminology",
    "href": "customization.html#terminology",
    "title": "Customization",
    "section": "Terminology",
    "text": "Terminology\nBefore starting your pre-processing and analysis, it’s important to understand a few core terms. These concepts are essential for working with text data and deciding on the appropriate pre-processing and analysis steps.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nTokenization\nBreaking text into units (words, phrases, sentences)\n\n\nStemming\nReducing words to their root (e.g., “running” → “run”)\n\n\nLemmatization\nReducing to dictionary form (e.g., “better” → “good”)\n\n\nStop Words\nCommon words often removed (e.g., “the”, “and”)\n\n\nDocument-Term Matrix\nTable of word frequencies across documents\n\n\nTF-IDF\nTerm importance based on frequency and inverse document frequency\n\n\nCorpus\nCollection of text documents as one dataset\n\n\nDictionary\nList of keywords/phrases used to code or categorize text\n\n\n\n\n\n\n# Load all packages \nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(textstem)\nlibrary(sentimentr)\nlibrary(topicmodels)\nlibrary(flextable)\nlibrary(knitr)\nlibrary(omni)\n\n\n\n\n\nPre-processing prepares your qualitative data for analysis by cleaning, organizing, and standardizing text. This step ensures the data is usable for word frequency, sentiment analysis, topic modeling, and other text mining techniques.\nYou should always assess the scope and quality of your data first:\n\nAre the data cleaned (lowercase, removed participant names, punctuation, symbols)?\n\n\n\n\n\n\n\nCheck out our function that does all of these data cleaning tasks for\nyou in R! You can also segment your text by question, combine multiple\ninterviews and focus groups into one dataset, and more.\n\n\n\n\n Cleaning transcripts function \n\n\nHow much text do you have? (Word counts per document/response.)\nHow many participants contributed?\nHow detailed or shallow are the responses?\nWhose voices are you analyzing?\n\nOnce you understand your data, you can decide which pre-processing steps are appropriate."
  },
  {
    "objectID": "customization.html#tokenization",
    "href": "customization.html#tokenization",
    "title": "Customization",
    "section": "Tokenization",
    "text": "Tokenization\nTokenization breaks text into individual pieces—usually words, but sometimes phrases or sentences.\n\nMost text mining techniques require tokenized text.\nThis is common for word frequency analysis, sentiment analysis, and topic modeling.\nAfter tokenization, we lose the flow of sentences which is generally okay for some analyses, but not for narrative analysis or content analysis.\n\nWhat are Stop Words?\nStop words are common words like “the”, “and”, “but” that don’t add much meaning on their own. Because we want to focus on broader themes from our participants’ voices, we usually want to remove stop words after text is tokenized. This process allows us to focus on content-rich words, like nouns and verbs. We should remove stop words when running a word frequency analysis, topic modeling, or sentiment analysis.\nWe can also create custom stop words that we want to remove from our analyses, for example, location names or project-specific words that appear frequently but aren’t relevant to the analysis.\nExample:\n\nIn the example below, my custom stop words are “travis” and “county”, and they’re added to the stop_words bank which includes other words like “but”, “is”, “the”, etc. If you’d like to see all of the stop words, see View(stop_words)."
  },
  {
    "objectID": "customization.html#stemming",
    "href": "customization.html#stemming",
    "title": "Customization",
    "section": "Stemming",
    "text": "Stemming\nStemming words cuts them down to their root forms (e.g., “running” becomes “run”). This pre-processing step should primarily be used for topic modeling or for other analyses that you decide that exact word form isn’t critical. This step is recommended to use with larger datasets, with typically hundreds of documents or more, but for our purposes we recommend stemming any time you have enough data for topic modeling (around 5,000 total words)."
  },
  {
    "objectID": "customization.html#lemmatization",
    "href": "customization.html#lemmatization",
    "title": "Customization",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization converts words to their dictionary root and keeps the words readable to the user (e.g., “better” becomes “good”). This pre-processing step should be used for word frequency analysis, sentiment analysis, or thematic/content analysis as it works will with small and large corpuses.\nLet’s compare the stem words, vs lemmatized words against the original tokenized words:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nline\nspeaker\nparticipant_id\nquestion\nsection\nsection_label\nsession\nword\nstem_word\nlem_word\n\n\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\nbiggest\nbiggest\nbig\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\nchallenges\nchalleng\nchallenge\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\ntransportation\ntransport\ntransportation\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\npeople\npeopl\npeople\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\npeople\npeopl\npeople\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\npeople\npeopl\npeople\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\npeople\npeopl\npeople\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\ncars\ncar\ncar\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\nmakes\nmake\nmake\n\n\n2\nparticipant\nP1\n0\n1\nIntroduction\nEducation\ndifficult\ndifficult\ndifficult\n\n\n\n\n\n\n\n\nWhen to Use Stemming vs. Lemmatization\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nYou need speed over accuracy\nSlower but more accurate\n\n\nWorking with large datasets\nBetter for small datasets\n\n\nBasic information retrieval tasks\nBetter for nuanced analysis\n\n\nSearch engines or topic modeling where fine detail isn’t critical\nBetter for in-depth analysis\n\n\nNot concerned about human readability\nMaintains readable words)\n\n\n\n\n\n\n\n  \n    Analyses"
  },
  {
    "objectID": "customization.html#word-frequency-analysis",
    "href": "customization.html#word-frequency-analysis",
    "title": "Customization",
    "section": "Word frequency analysis",
    "text": "Word frequency analysis\n\n\n\n\nWord frequency analysis is a simple and effective method that counts how often specific words appear in your dataset. It’s often used early in qualitative analysis to get a general sense of prominent topics, but it can also be applied as a stand-alone method to identify frequently mentioned issues in participant responses. Word frequency works best when you have a reasonable amount of text to analyze. As a general best practice, having at least 100–200 content words (excluding stop words) allows for more reliable interpretation of patterns, though smaller datasets can still offer preliminary insights.\nTo ensure accuracy, it’s important to pre-process your text. Using lemmatized words helps avoid counting variations like “run” and “running” separately, and removing stop words—common words such as “the” or “and”—allows you to focus on terms that carry more meaning. In cases where participants mention names of places or projects repeatedly, custom stop word lists can also help refine the results.\n\n\nWhile word frequency counts themselves do not explain context or meaning, they can help point to emerging themes by highlighting concepts that recur across participants or within particular sections of discussion. For example, if words like “transportation,” “access,” and “barrier” frequently appear in responses about service challenges, they signal a potential theme that warrants deeper exploration. Word frequency analysis can also help guide more interpretive methods such as thematic or content analysis, and is most useful when findings are contextualized within the broader dataset and research goals.\n\n\n# A tibble: 6 × 2\n  word             n\n  &lt;chr&gt;        &lt;int&gt;\n1 time             4\n2 people           3\n3 access           2\n4 application      2\n5 applications     2\n6 process          2"
  },
  {
    "objectID": "customization.html#sentiment-analysis",
    "href": "customization.html#sentiment-analysis",
    "title": "Customization",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nSentiment analysis is a method that measures the emotional tone of text by categorizing words or phrases as positive, negative, or neutral. It can be used to quickly gauge participants’ attitudes toward a topic or to assess the overall tone of responses across interviews, focus groups, or surveys. Sentiment analysis works best when you have larger amounts of text—ideally, datasets with several hundred words or more—because emotional tone can vary within short responses, making sentiment harder to interpret in very small datasets. However, it can still offer insights in smaller datasets when applied carefully and when results are presented as exploratory.\nBest practices for sentiment analysis include pre-processing your text by removing stop words and standardizing language through lemmatization. This ensures consistent scoring and avoids misclassifications due to word variations. Sentiment analysis typically relies on pre-built lexicons, such as Bing, AFINN, or the NRC Emotion Lexicon, which assign emotional values to words. It’s important to note that these lexicons were often designed for general use (such as social media text) and may require customization to fit specific public health or social science contexts. For example, in health-related interviews, a word like “treatment” might appear frequently and carry different sentiment depending on the discussion’s focus.\nWhile sentiment analysis doesn’t capture nuance or context in the way manual coding can, it can highlight patterns of emotional tone across datasets or within specific discussion topics. For example, sentiment analysis might reveal that participants express more negative sentiment when discussing barriers to accessing services, and more positive sentiment when discussing recommendations for future improvements. These insights can help guide deeper qualitative coding or serve as an additional layer of analysis to support findings. As with any automated method, it’s important to review and interpret results in context and to document any limitations or adaptations made to the analysis.\n\n\n# A tibble: 2 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative      5\n2 positive      3\n\n\n\n\n\n\n\n\n\nTip for finding quotes by tone\nLet’s say you want to pull a quote to include in a report and you know that you want it to be a positively toned quote in a “recommendations” section of your discussion. You can use the sentimentr package to gather sentiments of each sentence (or segment) and sort by sentiment score to find the quote with the most positive tone in your criteria.\nSee an example below:\n\n\n# A tibble: 7 × 12\n   line speaker     participant_id question section section_label  text  session\n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;  \n1     2 participant P1                    0       1 Introduction   sure… Educat…\n2     3 participant P2                    0       1 Introduction   i ag… Educat…\n3     5 participant P1                    0       2 Barriers       trav… Educat…\n4     6 participant P2                    0       2 Barriers       also… Public…\n5     8 participant P1                    0       3 Recommendatio… more… Public…\n6     9 participant P2                    0       3 Recommendatio… simp… Public…\n7    10 participant P1                    0       3 Recommendatio… prov… Public…\n# ℹ 4 more variables: element_id &lt;int&gt;, avg_sentiment &lt;dbl&gt;,\n#   sd_sentiment &lt;dbl&gt;, n_sentences &lt;int&gt;\n\n\nMixed-methods with sentiment analysis\nBecause sentiment analysis gives you a numeric output as a sentiment score, you can imagine instances where you may want to compare sentiment scores between groups, correlate sentiment scores with other variables in a quantitative survey, or conduct pre-post comparisons."
  },
  {
    "objectID": "customization.html#topic-modeling",
    "href": "customization.html#topic-modeling",
    "title": "Customization",
    "section": "Topic modeling",
    "text": "Topic modeling\nTopic modeling is an automated method used to identify themes or topics across large collections of text. It uses algorithms to group together words that frequently appear in similar contexts, helping reveal hidden patterns or structures in qualitative data. Topic modeling is especially useful when you have large datasets—typically a minimum of 5,000 words or more spread across multiple documents or participant responses. The method works best when documents are of relatively similar length, which helps the model assign topics more evenly.\nThis method is not recommended for use on smaller corpuses and would be considered poor practice to rely on it to generate meaningful insights. If you don’t have enough data for this analysis, consider using the content analysis strategy to identify themes in the data.\nBefore running a topic model, it’s important to pre-process your text. Best practices include stemming words to reduce variation (so that “run” and “running” are treated as the same word) and removing stop words to focus on meaningful content. Topic modeling doesn’t require predefined codes or themes, making it a good exploratory tool for surfacing unexpected topics in your data. However, it’s important to remember that these topics are generated algorithmically—they group terms based on statistical patterns, not human interpretation. As a result, human review is always needed to interpret and label the topics in a way that makes sense for your project and participants.\nTopic modeling can complement manual coding by offering a high-level view of common themes, pointing analysts toward areas that may warrant deeper exploration. For example, a topic model might surface clusters of words related to barriers (“transportation,” “access,” “cost”) and another cluster about solutions (“education,” “outreach,” “support”), giving you a starting point for thematic analysis. Commonly used R packages for topic modeling include topicmodels, which provides algorithms like Latent Dirichlet Allocation (LDA), and tm for pre-processing and managing textual data.\n\n\n[1] 46.1033\n\n\n[1] 46.13593\n\n\n# A tibble: 20 × 3\n# Groups:   topic [2]\n   topic term             beta\n   &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1     1 time           0.0617\n 2     1 transportation 0.0544\n 3     1 services       0.0467\n 4     1 residents      0.0313\n 5     1 challenges     0.0297\n 6     1 agree          0.0281\n 7     1 lot            0.0278\n 8     1 access         0.0276\n 9     1 applications   0.0275\n10     1 application    0.0274\n11     2 people         0.0800\n12     2 time           0.0695\n13     2 process        0.0485\n14     2 application    0.0381\n15     2 applications   0.0381\n16     2 access         0.0380\n17     2 timeconsuming  0.0264\n18     2 campaigns      0.0260\n19     2 buses          0.0254\n20     2 simplifying    0.0253\n\n\n# A tibble: 30 × 3\n# Groups:   topic [3]\n   topic term             beta\n   &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1     1 time           0.0778\n 2     1 transportation 0.0543\n 3     1 services       0.0542\n 4     1 lot            0.0390\n 5     1 residents      0.0353\n 6     1 applications   0.0347\n 7     1 access         0.0312\n 8     1 vouchers       0.0261\n 9     1 makes          0.0256\n10     1 planning       0.0249\n# ℹ 20 more rows"
  },
  {
    "objectID": "customization.html#content-analysis",
    "href": "customization.html#content-analysis",
    "title": "Customization",
    "section": "Content analysis",
    "text": "Content analysis\nContent analysis is a method used to count how often predefined concepts, themes, or categories appear in qualitative data. It relies on a dictionary—a list of key terms or phrases—designed to reflect your evaluation questions or coding framework. Content analysis works well for both small and large datasets, making it a flexible tool when you want to systematically measure the presence of specific themes across interviews, focus groups, or open-ended survey responses.\nA key requirement for effective content analysis is a carefully designed dictionary that accurately captures the concepts you’re interested in. This might include terms related to barriers, facilitators, or recommendations, depending on the project’s goals. Best practice is to validate the dictionary by reviewing examples of matched text to make sure the terms are identifying the intended content. You may need to refine the dictionary over time, adding synonyms or removing words that generate false positives.\nContent analysis can help answer questions like “How frequently do participants mention prevention strategies?” or “What percentage of responses reference funding challenges?” It is particularly useful when you need to quantify qualitative data for reporting purposes, or when you want to compare how frequently themes appear across different stakeholder groups. In R, the quanteda package offers efficient tools for dictionary-based content analysis, allowing you to apply a dictionary and quickly summarize how often key terms or concepts appear in the dataset.\n\n\nDocument-feature matrix of: 7 documents, 109 features (81.26% sparse) and 0 docvars.\n       features\ndocs    sure i think one of the biggest challenges is transportation\n  text1    1 1     1   1  5   2       1          1  1              1\n  text2    0 1     0   0  0   0       0          0  0              0\n  text3    0 0     0   0  1   0       0          0  0              0\n  text4    0 0     0   0  0   4       0          0  1              0\n  text5    0 0     0   0  0   0       0          0  0              0\n  text6    0 0     0   0  0   1       0          0  0              0\n[ reached max_ndoc ... 1 more document, reached max_nfeat ... 99 more features ]\n\n\nDocument-feature matrix of: 7 documents, 3 features (85.71% sparse) and 0 docvars.\n       features\ndocs    prevention treatment harm_reduction\n  text1          0         1              0\n  text2          0         0              0\n  text3          1         0              0\n  text4          0         0              0\n  text5          2         0              0\n  text6          0         0              0\n[ reached max_ndoc ... 1 more document ]\n\n\n[1] \"text1\" \"text2\" \"text3\" \"text4\" \"text5\" \"text6\"\n\n\n# A tibble: 6 × 12\n   line speaker     participant_id question section section_label  text  session\n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;  \n1     2 participant P1                    0       1 Introduction   sure… Educat…\n2     3 participant P2                    0       1 Introduction   i ag… Educat…\n3     5 participant P1                    0       2 Barriers       trav… Educat…\n4     6 participant P2                    0       2 Barriers       also… Public…\n5     8 participant P1                    0       3 Recommendatio… more… Public…\n6     9 participant P2                    0       3 Recommendatio… simp… Public…\n# ℹ 4 more variables: doc_id &lt;chr&gt;, prevention &lt;dbl&gt;, treatment &lt;dbl&gt;,\n#   harm_reduction &lt;dbl&gt;"
  },
  {
    "objectID": "customization.html#thematic-analysis-in-dedoose",
    "href": "customization.html#thematic-analysis-in-dedoose",
    "title": "Customization",
    "section": "Thematic Analysis in Dedoose",
    "text": "Thematic Analysis in Dedoose\n@qualBPT help here!! Can we add anything re coding to this guide in this section, maybe Ivonne’s work? What other programs should we add?\nIf using Dedoose to conduct thematic analysis, we recommend a structured, reflective approach grounded in reflexive thematic analysis (Clarke & Braun, 2006) and informed by critical realism.\nHere’s how to approach it:\n\nStart with Familiarization Read through transcripts or responses in full before coding.\n\nUse memos to reflect on initial impressions, patterns, and researcher assumptions.\n\nDevelop Codes Iteratively Begin with inductive coding (bottom-up), focusing on what participants actually say.\n\nAvoid pre-loading the codebook unless doing deductive analysis is needed for evaluation purposes.\nUse Dedoose’s “Descriptor” fields to track context (e.g., stakeholder group, session type) for later analysis.\n\nApply Codes Reflexively Apply codes carefully, updating definitions and merging/splitting codes as needed.\n\nEncourage coders to discuss disagreements and maintain an audit trail of key coding decisions.\n\nConstruct Themes Thoughtfully After coding, review excerpts by code and group them into broader themes that reflect patterns across participants.\n\nUse Dedoose’s “Code Co-Occurrence” and “Code Application” tools to explore theme structure.\n\nAcknowledge Researcher Role Reflect on how your perspective, language, and interpretation influence theme development.\n\nBe cautious not to overstate findings; describe patterns and nuance rather than asserting consensus.\n\nDocument Everything Keep a record of how codes and themes evolved.\n\nClearly state the number of participants or excerpts that informed each theme.\nMake your interpretive lens and limitations explicit—especially when datasets are small or uneven across groups.\n\nThematic analysis in Dedoose should still reflect Omni’s commitment to critical realism: treat participant input as reflecting real issues shaped by context and perspective, and be transparent about what the data can—and cannot—support."
  },
  {
    "objectID": "customization.html#combining-analyses",
    "href": "customization.html#combining-analyses",
    "title": "Customization",
    "section": "Combining analyses",
    "text": "Combining analyses\nNo single method can fully capture the richness and complexity of qualitative data. At Omni, we often combine different qualitative analysis approaches to explore various angles of our data and answer nuanced evaluation questions. Pairing methods like word frequency, sentiment analysis, content analysis, and thematic analysis can help reveal both patterns and meaning, ensuring our findings are grounded in evidence and provide actionable insights."
  },
  {
    "objectID": "subtab1.html",
    "href": "subtab1.html",
    "title": "Subtab1 test",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "subtab1.html#quarto",
    "href": "subtab1.html#quarto",
    "title": "Subtab1 test",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "subtab1.html#running-code",
    "href": "subtab1.html#running-code",
    "title": "Subtab1 test",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  }
]