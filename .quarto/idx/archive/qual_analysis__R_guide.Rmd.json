{"title":"Qualitative Analysis Guide","markdown":{"yaml":{"title":"Qualitative Analysis Guide","subtitle":"","author":"Hannah","date":"`r Sys.Date() |> format('%B %d, %Y')`","output":{"omni::html_report":{"toc":true,"toc_float":true,"main_font":"Inter Tight","background_color":"#f9f7f4","remove_logo":false}}},"headingText":"Key Features of the Guide:","containsRefs":false,"markdown":"\n\n```{r, echo=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)\n```\n\n```{r, echo=FALSE}\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(omni)\nlibrary(scales)\n```\n\n```{=html}\n<style>\n  .outer {\n    display: block;\n    width: 100%;\n  }\n\n  .green-box {\n    --box-h: 80px;                     /* height of the bar */\n    --bar-bg: #41816F;                 /* default background color */\n    --bar-text-color: #ffffff;         /* default text color */\n    --pattern-img: url(\"images/Pattern-01-Teal.png\");\n\n    height: var(--box-h);\n    width: 100%;\n    display: flex;\n    background-color: var(--bar-bg);\n    color: var(--bar-text-color);\n    border-radius: 0;\n    overflow: visible;\n  }\n\n  .green-col {\n    flex: 1 1 auto;\n    display: flex;\n    align-items: center;\n    padding-left: 1em;\n    font-size: 3.4rem;\n    font-weight: 300;\n    color: var(--bar-text-color);\n    white-space: nowrap;\n  }\n\n  .green-col.pattern {\n    flex: 0 0 auto;\n    padding: 0 !important;\n    display: flex;\n    align-items: center;\n    justify-content: flex-end;\n    background: none;\n  }\n\n  .green-col.pattern img {\n    height: 100%;\n    width: auto;\n    display: block;\n  }\n</style>\n```\n\n```{=html}\n<style>\n.yellow-btn {\n  display: inline-block;\n  background-color: #fcd82b;\n  color: #000000;\n  padding: 0.5em 1.2em;\n  text-decoration: none;\n  border: none;\n  border-radius: 0px;   /* squared edges? change to 0 */\n  font-weight: bold;\n  font-size: 1.5rem;\n}\n.yellow-btn:hover {\n  background-color: #e6c323; /* slightly darker on hover */\n}\n</style>\n```\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #41816F;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Executive Summary</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-01-Teal.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<div style=\"background-color: #c5dfd9; padding: 20px; border-radius: 0px;\">\n  <p>Omni's qualitative analysis guide supports staff in conducting transparent, systematic, and actionable qualitative research in complex, real-world settings. Our clients often require timely, credible findings grounded in participants' lived experiences. To meet these demands, this guide outlines practical workflows rooted in two complementary paradigms: **[pragmatism]{.teal-600}** and **[critical realism]{.teal-600}**.</p>\n\n  <ul>\n    <li>**[Pragmatism]{.teal-600}** directs us to focus on real-world utility and actionable insights.</li>\n    <li>**[Critical realism]{.teal-600}** encourages us to understand participant perspectives while acknowledging how those perspectives are shaped by broader structures and contexts.</li>\n  </ul>\n\n  <p>Together, these paradigms support our commitment to meaningful, context-aware findings that are both usable and methodologically sound.</p>\n\n  <p>This guide provides step-by-step instructions for conducting qualitative analysis, from data pre-processing to reporting. It includes techniques like word frequency analysis, sentiment analysis, thematic coding, content analysis, and topic modeling, all implemented with transparency and adaptability in mind.</p>\n\n</div>\n\n\n```{r, echo=FALSE}\nwidth_px <- 40\ntext_width <- 800\nicon_color_fg <- \"white\"\nicon_color_bg <- 'teal-400'\n\nicon_text_grid(\n    omni_icon('education', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        text = '<strong>Tool Flexibility:</strong> While R is our preferred tool for reproducibility and integration with quantitative methods, the guide also includes best-practice workflows for using Dedoose (for team-based coding) and AI tools like NotebookLM (for early exploratory work).',\n        text_width\n    ),\n    omni_icon('security', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        '<strong>Method Selection Guidance:</strong> We offer practical decision points based on dataset size, analytic goals, and project context, ensuring methods match the scope and purpose of each evaluation.',\n        text_width\n    ),\n    omni_icon('vault', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        '<strong>Structured Workflows:</strong> Omni workflows help analysts navigate all stages of qualitative analysis—from clarifying analytic frameworks and coding approaches to integrating qualitative and quantitative data in mixed-methods designs.',\n        text_width\n    ),\n    omni_icon('vault', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        '<strong>Adaptability:</strong> Whether you\\'re working with rich interview transcripts or brief open-ended survey responses, the guide provides adaptable tools that emphasize rigor, transparency, and context.',\n        text_width\n    ),\n    column_gap_px = 10, # default is 10\n    row_gap_px = 10, # default is 10\n    width_px = width_px\n)\n```\n\n<br>\n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"By centering participant voices, documenting our methods clearly, and maintaining a reflexive stance, this guide strengthens the credibility and usefulness of Omni's qualitative work. It equips staff with tools and strategies that not only meet evaluation standards but also foster accountability, responsiveness, and connection to the communities we serve.\",\n    color = \"teal-600\",\n    fixed_width_px = 800\n)\n```\n\n<br>\n\n<details>\n<summary><strong>Version Control</strong></summary>\n\n| Version | Description              | Date released |\n|---------|--------------------------|---------------|\n| -4.0    | Adding interactive features   | 2025-08-14    |\n| -5.0    | Second draft for review and rebrand   | 2025-08-14    |\n| -6.0    | Initial draft for review  | 2025-03-26    |\n\n</details>\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #C65A8B;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Introduction</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-02-Plum.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n::::: columns\n::: {.column width=\"80%\"}\nAt Omni, we conduct qualitative research in complex, real-world\nenvironments. Our clients and projects often require fast timelines,\nnuanced insights, and transparent methods that can stand up to external\nreview. To meet these needs, we use systematic, reproducible qualitative\nanalysis methods grounded in **[pragmatism]{.plum-600}** and **[critical realism]{.plum-600}**.\n:::\n\n::: {.column width=\"20%\"}\n![](images/analysis1.png)\n:::\n:::::\n\nThis approach ensures our findings are actionable and reflect the\nrealities and lived experiences of participants, while acknowledging the\ninfluence of context, interpretation, and limitations in our data.\n\n## Purpose {.unnumbered}\n\nThis guide was developed to support Omni staff in conducting\nmethodologically sound, transparent, and reproducible qualitative\nanalyses. It offers practical, step-by-step instructions and highlights\nbest practices for text pre-processing, analysis, and reporting. Whether\nworking with interview transcripts, focus groups, open-ended survey\nresponses, or other text-based data, Omni teams can use this guide to\nproduce findings that are both grounded in participants’ voices and\nuseful for program improvement and decision-making.\n\n#### How does this guide align with Omni's core values?\n\n```{=html}\n<style>\n  .values-row {\n    --card-size: 180px;        /* card width/height */\n    --plum: #921c4c;\n    --front-text: #ffffff;\n    --back-text: #2b2b2b;\n    display: grid;\n    grid-template-columns: repeat(4, var(--card-size));\n    gap: 16px;\n    justify-content: start;\n    align-items: start;\n  }\n\n  .card {\n    width: var(--card-size);\n    height: var(--card-size);\n    perspective: 1000px;\n    cursor: pointer;\n    border: none;\n    border-radius: 0;\n    padding: 0;\n    background: none;\n    outline: none; /* remove black outline entirely */\n  }\n\n  .card-inner {\n    position: relative;\n    width: 100%;\n    height: 100%;\n    transition: transform 420ms ease;\n    transform-style: preserve-3d;\n  }\n\n  .card:hover .card-inner,\n  .card.is-flipped .card-inner {\n    transform: rotateY(180deg);\n  }\n\n  .card-face {\n    position: absolute;\n    inset: 0;\n    display: grid;\n    place-items: center;\n    padding: 14px;\n    border-radius: 0;\n    backface-visibility: hidden;\n    text-align: center;\n  }\n\n  /* Bigger front font */\n  .card-front {\n    background: var(--plum);\n    color: var(--front-text);\n    font-weight: 700;\n    font-size: 1.8rem; /* larger than before */\n    letter-spacing: .3px;\n  }\n\n  /* Bigger back font */\n  .card-back {\n    background: #f5f3f2;\n    color: var(--back-text);\n    transform: rotateY(180deg);\n    font-size: 1.5rem; /* larger text */\n    line-height: 1.35;\n  }\n</style>\n\n<div class=\"values-row\">\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Inquiry</div>\n      <div class=\"card-face card-back\">\n        Provides clear, rigorous methods that encourage thoughtful exploration of participants’ experiences.\n      </div>\n    </div>\n  </button>\n\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Agility</div>\n      <div class=\"card-face card-back\">\n        Equips staff with adaptable tools to meet diverse project needs and respond efficiently to change.\n      </div>\n    </div>\n  </button>\n\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Connection</div>\n      <div class=\"card-face card-back\">\n        Centers participants’ voices and strengthens relationships with the communities and partners we serve.\n      </div>\n    </div>\n  </button>\n\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Accountability</div>\n      <div class=\"card-face card-back\">\n        Uses transparent, reproducible analysis practices to keep our work credible, ethical, and trustworthy.\n      </div>\n    </div>\n  </button>\n</div>\n\n<script>\n  document.querySelectorAll('.card').forEach(card => {\n    card.addEventListener('click', () => card.classList.toggle('is-flipped'));\n    card.addEventListener('keydown', (e) => {\n      if (e.key === 'Enter' || e.key === ' ') {\n        e.preventDefault();\n        card.classList.toggle('is-flipped');\n      }\n    });\n  });\n</script>\n```\n\n## What This Guide Covers {.unnumbered}\n\nYou’ll find guidance on:\n\n-   **[Text mining / Natural Language Processing (NLP)]{.plum-600}**\\\n    (e.g., word frequency, sentiment analysis, topic modeling)\n\n-   **[Thematic and content analysis]{.plum-600}**\\\n    (e.g., dictionary-based coding, thematic coding frameworks)\n\n-   **[Narrative analysis @qualBPT do we need?]{.plum-600}**\\\n    (Optional: e.g., structural analysis, language style matching)\n\n-   **[Visualizing and reporting qualitative data]{.plum-600}**\\\n    (e.g., word clouds, bar charts, heatmaps, joint displays for\n    mixed-methods)\n\nThese methods can be applied to text from: Word documents (.docx), PDFs\n(.pdf), Zoom transcripts (.docx or .txt), open-ended survey responses\n(.xlsx or .csv), and more.\n\nIn many applied settings, qualitative methods (and especially\nmixed-methods) are described in vague terms. Reports may mention \"themes\nemerging\" or \"triangulating findings,\" but rarely explain the actual\nprocess used to get there. This vagueness makes it hard to replicate or\nassess qualitative findings and limits their credibility.\n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"At Omni, we aim to avoid vague or ad hoc practices. We document clear, systematic workflows that integrate qualitative insights with\nquantitative data when appropriate.\",\n    color = \"plum-600\",\nfixed_width_px = 800\n)\n```\n\n### Why Are Existing Descriptions of Methods So Vague? {.unnumbered}\n\nSeveral factors explain the common lack of clarity in methodology:\n\n-   Different disciplines (e.g., public health vs. education research)\n    use different frameworks, leading to inconsistent approaches.\n\n-   Quantitative research has standard tools (R, SPSS, Stata), but\n    qualitative tools (NVivo, MAXQDA, Dedoose) often rely on manual\n    processes and don't always integrate cleanly with quantitative\n    workflows.\n\n-   Qualitative analysis requires human interpretation and reflexivity.\n    Documenting every step is time-consuming, and under tight timelines,\n    many organizations skip this critical process.\n\n-   Qualitative and quantitative teams often work separately, without\n    shared standards or mixed-methods integration.\n\n[Hannah is just messing around here to see how an .mp4 reads into R and plays]\n```{=html}\n<video width=\"640\" height=\"360\" controls>\n  <source src=\"videos/test Video Project 1.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n```\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #89A046;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Omni's Approach to Analysis</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-03-Olive.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\nOmni’s qualitative practice is grounded in **[pragmatism]{.olive-green-600}** and **[critical realism]{.olive-green-600}**.\n\n-   **[Pragmatism]{.olive-green-600}** means we focus on providing useful, actionable insights for real-world decision-making. Our goal is to help clients\n    understand participants’ experiences in ways that inform policy,\n    programs, and practice. This reflects a pragmatic stance rooted in\n    the work of scholars like <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/2345678906292462\" target=\"_blank\" rel=\"noopener\">\n      Morgan (2007)\n    </a>\n    and <a href=\"https://www.taylorfrancis.com/books/mono/10.4324/9781351017831/explaining-society-berth-danermark-mats-ekstr%C3%B6m-jan-ch-karlsson\" target=\"_blank\" rel=\"noopener\">\n      Danermark et al. (2015)\n    </a>, emphasizing inquiry that is guided by consequences and utility.\n\n-   **[Critical realism]{.olive-green-600}** acknowledges that while participants’ experiences reflect real phenomena (barriers, challenges,\n    successes), they are shaped by context and perspective. Influenced\n    by <a href=\"https://www.tandfonline.com/doi/full/10.1080/13645579.2016.1144401\" target=\"_blank\" rel=\"noopener\">\n      Fletcher’s (2017)\n    </a> work and work by others, this perspective guides us to acknowledge\n    our limitations, reflect on researcher influence, and avoid\n    overstating claims.\n\n-   Omni’s qualitative practice draws on reflexive thematic analysis\n  <a href=\"https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa\" target=\"_blank\" rel=\"noopener\">\n    (Clarke &amp; Braun, 2006)\n  </a>\n  and content analysis\n  <a href=\"https://journals.sagepub.com/doi/abs/10.1177/1049732305276687\" target=\"_blank\" rel=\"noopener\">\n    (Hseih &amp; Shannon, 2005)\n  </a>,\n  emphasizing the researcher’s active role in interpreting data and\n  constructing themes, in alignment with our commitment to critical\n  realism and methodological transparency.\n\n### Transparent and Systematic Workflows {.unnumbered}\n\nAt Omni, we use structured qualitative and mixed-methods workflows.\nThese processes are transparent, systematic, and designed to integrate\nqualitative and quantitative findings when appropriate.\n\n![](images/workflow_green.png)\n\n**Example Omni Workflow for Qualitative analysis:**\n\n1.  Clarify Purpose and Analytic Framework\n\n  -  Define the evaluation questions and purpose of the qualitative\n    analysis\n\n  -  Select an analytic method\n\n2.  Prepare and Familiarize with the Data\n\n  -  Clean transcripts/text\n\n  -  Read through interviews, discussions, or sessions at least once\n    (text like reports you may not need context for, use your\n    discretion)\n\n3.  Systematic Coding of the Data\n\n  -  Deductive, Inductive, or hybrid\n\n  -  Deductive coding uses predefined codes (like our dictionaries) to\n    apply to our data\n\n  -  Inductive coding uses text mining or topic modeling to identify\n    emergent themes\n\n4.  Theme Development and Refinement\n\n  -  Analyze and refine codes/themes\n\n5.  Interpretation and Synthesis\n\n  -  Compare themes to evaluation questions and contextual frameworks\n\n6.  Explore themes visually\n\n7.  Get ready for reporting\n\n## Choosing Your Analysis Tool: R vs. Dedoose vs. AI {.unnumbered}\n\n```{=html}\n<style>\n  /* ---- Carousel theme ---- */\n  .omni-carousel {\n    --bg: #89A046;          /* slide background */\n    --title: #ffffff;       /* slide title color */\n    --body: #000000;        /* body text color */\n    --accent: #2b2b2b;      /* arrows/dots */\n    position: relative;\n    max-width: 900px;       /* set a fixed width if you like */\n    margin: 12px auto 24px;\n    overflow: hidden;\n    border: 0;              /* squared edges */\n  }\n\n  .oc-track {\n    display: grid;\n    grid-auto-flow: column;\n    grid-auto-columns: 100%;\n    transition: transform 380ms ease;\n  }\n\n  .oc-slide {\n    background: var(--bg);\n    color: var(--body);\n    min-height: 280px;\n    display: grid;\n    padding: 20px 22px;\n    align-content: start;\n    gap: 10px;\n  }\n\n  .oc-title {\n    color: var(--title);\n    font-size: 1.6rem;\n    font-weight: 800;\n    letter-spacing: .2px;\n    margin: 0 0 2px 0;\n  }\n\n  .oc-cols {\n    display: grid;\n    grid-template-columns: 1fr 1fr;\n    gap: 16px;\n  }\n\n  .oc-tag {\n    display: inline-block;\n    font-weight: 700;\n    padding: 2px 8px;\n    background: rgba(255,255,255,0.75);\n    border: 0;\n  }\n\n  .oc-list {\n    margin: 6px 0 0 0;\n    padding-left: 18px;\n  }\n  .oc-list li { margin: 4px 0; }\n\n  /* Arrows */\n  .oc-arrow {\n    position: absolute;\n    top: 50%;\n    translate: 0 -50%;\n    background: #fff;\n    color: var(--accent);\n    border: 2px solid var(--accent);\n    padding: 6px 10px;\n    cursor: pointer;\n    font-weight: 800;\n  }\n  .oc-prev { left: 6px; }\n  .oc-next { right: 6px; }\n\n  /* Dots */\n  .oc-dots {\n    display: flex;\n    gap: 6px;\n    justify-content: center;\n    margin-top: 8px;\n  }\n  .oc-dot {\n    width: 10px; height: 10px; border-radius: 50%;\n    background: #d1d5db; border: 0; cursor: pointer;\n  }\n  .oc-dot[aria-current=\"true\"] { background: var(--accent); }\n\n  /* Responsive */\n  @media (max-width: 720px) {\n    .oc-cols { grid-template-columns: 1fr; }\n    .oc-title { font-size: 1.35rem; }\n  }\n</style>\n\n<div class=\"omni-carousel\" role=\"region\" aria-label=\"Ways of analyzing qualitative data\">\n  <div class=\"oc-track\" data-index=\"0\" aria-live=\"polite\">\n\n    <!-- Slide: R -->\n    <section class=\"oc-slide\" data-name=\"R\">\n      <h3 class=\"oc-title\">R</h3>\n      <div class=\"oc-cols\">\n        <div>\n          <span class=\"oc-tag\">Best when</span>\n          <ul class=\"oc-list\">\n            <li>Datasets are large or mixed (qual + quant), need audit trails.</li>\n            <li>You want reproducibility, automation, version control.</li>\n            <li>Time crunch with existing scripts/templates; IRR optional or phased.</li>\n          </ul>\n        </div>\n        <div>\n          <span class=\"oc-tag\">Pros</span>\n          <ul class=\"oc-list\">\n            <li>Transparent, reproducible code; scalable pipelines.</li>\n            <li>Flexible: word freq, sentiment, dictionaries, topic models.</li>\n            <li>Precise visuals; integrates surveys/demographics easily.</li>\n          </ul>\n          <span class=\"oc-tag\" style=\"margin-top:8px;\">Cons</span>\n          <ul class=\"oc-list\">\n            <li>Setup/learning curve.</li>\n            <li>Requires clear documentation to ensure shared understanding.</li>\n          </ul>\n        </div>\n      </div>\n    </section>\n\n    <!-- Slide: Dedoose -->\n    <section class=\"oc-slide\" data-name=\"Dedoose\">\n      <h3 class=\"oc-title\">Dedoose</h3>\n      <div class=\"oc-cols\">\n        <div>\n          <span class=\"oc-tag\">Best when</span>\n          <ul class=\"oc-list\">\n            <li>Collaborative coding with a team; training new coders.</li>\n            <li>Small–moderate corpora.</li>\n            <li>Quick turnarounds with IRR workflows built in the tool.</li>\n          </ul>\n        </div>\n        <div>\n          <span class=\"oc-tag\">Pros</span>\n          <ul class=\"oc-list\">\n            <li>Multi‑user collaboration, role permissions, IRR features.</li>\n            <li>Low barrier for non‑programmers.</li>\n          </ul>\n          <span class=\"oc-tag\" style=\"margin-top:8px;\">Cons</span>\n          <ul class=\"oc-list\">\n            <li>License cost.</li>\n            <li>Automation/custom analyses limited vs. code.</li>\n            <li>Reproducibility/export trails less granular than scripted pipelines.</li>\n          </ul>\n        </div>\n      </div>\n    </section>\n\n    <!-- Slide: AI -->\n    <section class=\"oc-slide\" data-name=\"AI\">\n      <h3 class=\"oc-title\">AI</h3>\n      <div class=\"oc-cols\">\n        <div>\n          <span class=\"oc-tag\">Best when</span>\n          <ul class=\"oc-list\">\n            <li>Early exploration: surfacing possible themes/codes quickly.</li>\n            <li>Short corpora; scoping questions; drafting a codebook.</li>\n            <li>Stakeholder previews before deeper human analysis.</li>\n          </ul>\n        </div>\n        <div>\n          <span class=\"oc-tag\">Pros</span>\n          <ul class=\"oc-list\">\n            <li>Very fast; low barrier; helps brainstorm structures.</li>\n            <li>Can summarize, cluster, and compare at a glance.</li>\n            <li>Useful for iteration before moving to R or Dedoose.</li>\n          </ul>\n          <span class=\"oc-tag\" style=\"margin-top:8px;\">Cons</span>\n          <ul class=\"oc-list\">\n            <li>Not a substitute for human interpretation; can hallucinate.</li>\n            <li>Inconsistent across runs; shallow context if prompts are weak.</li>\n            <li>Data governance/privacy limits—avoid sensitive uploads without approvals.</li>\n          </ul>\n        </div>\n      </div>\n    </section>\n\n  </div>\n\n  <!-- Controls -->\n  <button class=\"oc-arrow oc-prev\" aria-label=\"Previous slide\">‹</button>\n  <button class=\"oc-arrow oc-next\" aria-label=\"Next slide\">›</button>\n\n  <!-- Dots -->\n  <div class=\"oc-dots\" role=\"tablist\" aria-label=\"Carousel navigation\">\n    <button class=\"oc-dot\" aria-label=\"R\" aria-current=\"true\"></button>\n    <button class=\"oc-dot\" aria-label=\"Dedoose\"></button>\n    <button class=\"oc-dot\" aria-label=\"AI\"></button>\n  </div>\n</div>\n\n<script>\n(function(){\n  const track = document.querySelector('.oc-track');\n  const slides = Array.from(document.querySelectorAll('.oc-slide'));\n  const prev = document.querySelector('.oc-prev');\n  const next = document.querySelector('.oc-next');\n  const dots = Array.from(document.querySelectorAll('.oc-dot'));\n  let idx = 0, n = slides.length;\n\n  function go(i){\n    idx = (i + n) % n;\n    track.style.transform = `translateX(-${idx*100}%)`;\n    dots.forEach((d, j)=>d.setAttribute('aria-current', j===idx));\n  }\n\n  prev.addEventListener('click', ()=>go(idx-1));\n  next.addEventListener('click', ()=>go(idx+1));\n  dots.forEach((d, j)=>d.addEventListener('click', ()=>go(j)));\n\n  // Keyboard\n  document.querySelector('.omni-carousel').addEventListener('keydown', (e)=>{\n    if(e.key==='ArrowRight') go(idx+1);\n    if(e.key==='ArrowLeft')  go(idx-1);\n  });\n\n  // Touch swipe\n  let startX=null;\n  track.addEventListener('touchstart', e=>{ startX = e.touches[0].clientX; }, {passive:true});\n  track.addEventListener('touchmove', e=>{\n    if(startX===null) return;\n    const dx = e.touches[0].clientX - startX;\n    if(Math.abs(dx)>40){ go(idx + (dx<0 ? 1 : -1)); startX=null; }\n  }, {passive:true});\n})();\n</script>\n\n```\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #CC4100;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Method Selection</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-04-OrangeRed.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n::::: columns\n::: {.column width=\"80%\"}\nOnce pre-processed, your data is ready for analysis. Selecting the\n**[right method]{.orange-red-600}** depends on:\n\n-   How much data you have\n\n-   Your research question(s)\n\n-   Whether you need exploratory insights or answers to specific\n    questions\n\n-   Your integrative framework (if conducting mixed-methods)\n:::\n\n::: {.column width=\"20%\"}\n![](images/data.png)\n:::\n:::::\n\n### Understanding Your Data Before Analysis {.unnumbered}\n\nBefore starting pre-processing or analysis, it’s important to assess the\nscope and quality of your data. Take time to understand:\n\n-   How many participants are included?\n\n-   How much text do you have (word count, number of responses)?\n\n-   How detailed are the responses (in-depth vs. brief)?\n\n-   How are participants grouped (by stakeholder type, session, etc.)?\n\n-   Whose voices are most important to elevate in this analysis?\n\nThis step ensures you choose methods that fit your dataset and align\nwith your project goals. For example:\n\n-   Smaller datasets (under \\~2,000 words) are well-suited to word\n    frequency, sentiment analysis, or basic content analysis.\n\n-   Larger datasets (5,000+ words) can support topic modeling or\n    advanced thematic analysis.\n\nAssessing your data upfront helps set realistic expectations for\nanalysis and ensures your findings are grounded, transparent, and\ndefensible.\n\n### Working with a Small Corpus {.unnumbered}\n\nAt Omni, qualitative research often happens in real-world settings, with\ntight timelines and limited participant availability. We may aim for 10\ninterviews and complete 5. We may expect long, detailed conversations\nand instead get brief answers. Even with these constraints, small\ndatasets can still provide meaningful insights, especially when there is\nconsistency across participant experiences.\n\nWhen working with limited data:\n\n-   Focus on what participants actually shared, rather than attempting\n    to generalize.\n\n-   Document patterns and recurring concerns, while linking them clearly\n    to the project’s research questions.\n\n-   Be transparent about the number of participants, the methods used,\n    and any limitations in interpretation.\n\nSmaller samples can still highlight critical issues—such as barriers to\naccess or common recommendations for program improvement—but the scope\nand representativeness of these findings should be clearly communicated.\n\n### Suggested Methods by Dataset Size and Purpose {.unnumbered}\n\n<!-- | **Analysis** | **Typical Recommendation** | **Wiggle Room with Word Count** | **Why** | -->\n<!-- |-----------------|-----------------|-----------------|---------------------| -->\n<!-- | **Word Frequency** | 500+ words | ✅ High | Works even on **small datasets** for exploratory insights. You can analyze **under 500 words**, but findings may feel anecdotal rather than robust. Best if presented as observations rather than firm conclusions. | -->\n<!-- | **Sentiment Analysis** | 1,000+ words | ✅ Moderate | Sentiment varies widely within small datasets. Under **1,000 words**, **tone shifts** can skew results. You *can* use it in smaller datasets (500+ words), but you'll need **careful interpretation** and clear disclaimers. | -->\n<!-- | **Content Analysis (Dictionary-based) or Thematic Analysis** | 1,000+ words (small dictionaries) | ✅ Moderate | Works **well with small datasets**, especially if using **focused dictionaries** with clearly defined terms. Fewer words mean fewer opportunities for themes to emerge, so **results need contextualization**. | -->\n<!-- | **Topic Modeling** | 5,000+ words | ❌ Low | Topic modeling relies on **statistical probability**. Anything under **5,000 words** will **struggle to produce meaningful, stable topics**. While you *can* run a topic model on smaller datasets (e.g., 2,000 words), the **results are less trustworthy**, prone to overfitting, and often **hard to interpret**. Not recommended without strong caveats. | -->\n```{r echo=FALSE}\nlibrary(DT)\n\n# Your table data\ndf <- data.frame(\n  Analysis = c(\"Word Frequency\", \"Sentiment Analysis\", \n               \"Content Analysis (Dictionary-based) or Thematic Analysis\", \n               \"Topic Modeling\"),\n  Typical_Recommendation = c(\"500+ words\", \"1,000+ words\", \n                              \"1,000+ words (small dictionaries)\", \n                              \"5,000+ words\"),\n  Wiggle_Room_with_Word_Count = c(\"✅ High\", \"✅ Moderate\", \"✅ Moderate\", \"❌ Low\"),\n  Why = c(\n    \"Works even on small datasets for exploratory insights. You can analyze under 500 words, but findings may feel anecdotal rather than robust. Best if presented as observations rather than firm conclusions.\",\n    \"Sentiment varies widely within small datasets. Under 1,000 words, tone shifts can skew results. You can use it in smaller datasets (500+ words), but you'll need careful interpretation and clear disclaimers.\",\n    \"Works well with small datasets, especially if using focused dictionaries with clearly defined terms. Fewer words mean fewer opportunities for themes to emerge, so results need contextualization.\",\n    \"Topic modeling relies on statistical probability. Anything under 5,000 words will struggle to produce meaningful, stable topics. While you can run a topic model on smaller datasets (e.g., 2,000 words), the results are less trustworthy, prone to overfitting, and often hard to interpret. Not recommended without strong caveats.\"\n  ),\n  stringsAsFactors = FALSE\n)\n\ndatatable(\n  df,\n  escape = FALSE,  # lets you keep emoji and HTML formatting\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE\n  )\n)\n\n```\n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"<highlight>Note:</highlight> If you are working with open-ended responses from a survey\nand have data from <higlight>30 or fewer participants</higlight>, carefully consider\nwhether the data are sufficient for meaningful analysis. In many\ncases, it is appropriate to report that <higlight>there were too few responses to analyze systematically</higlight>. However, if you need to report findings, be transparent about limitations. Focus on <higlight>describing frequently mentioned key words</higlight>, rather than inferring broader themes. Avoid suggesting consensus when data are limited, and clearly state that findings reflect input from a <higlight>small number of participants</higlight>. Transparent reporting maintains the credibility of your analysis.\",\n    color = \"orange-red-600\",\n    fixed_width_px = 800\n)\n```\n\n### Grouping Voices to Guide Analysis {.unnumbered}\n\nBefore you start coding or analyzing, decide:\n\n-   Whose voices are we centering in this analysis?\n\n-   How will we group participant responses?\n\nYour grouping choices influence:\n\n-   Which perspectives are highlighted\n\n-   How themes emerge\n\n-   What questions you can answer\n\nAt Omni, we might group data by:\n\n-   Participant (individual experiences)\n\n-   Stakeholder Group (e.g., educators vs. public health professionals)\n\n-   Discussion Section (e.g., barriers vs. recommendations)\n\nThese decisions should align with the project’s goals and be clearly\ndocumented in reports and presentations.\n\n### Ready for Pre-processing? {.unnumbered}\n\nOnce you understand your data, you’re ready to start **pre-processing**!\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #5776b2;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Cleaning and Pre-Processing Your Data</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-05-Periwinkle.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n## Terminology {.unnumbered}\n\nBefore starting your pre-processing and analysis, it’s important to\nunderstand a few core terms. These concepts are essential for working\nwith text data and deciding on the appropriate pre-processing and\nanalysis steps.\n\n| **Term** | **Definition** |\n|-----------------------|-------------------------------------------------|\n| Tokenization | Breaking text into units (words, phrases, sentences) |\n| Stemming | Reducing words to their root (e.g., \"running\" → \"run\") |\n| Lemmatization | Reducing to dictionary form (e.g., \"better\" → \"good\") |\n| Stop Words | Common words often removed (e.g., \"the\", \"and\") |\n| Document-Term Matrix | Table of word frequencies across documents |\n| TF-IDF | Term importance based on frequency and inverse document frequency |\n| Corpus | Collection of text documents as one dataset |\n| Dictionary | List of keywords/phrases used to code or categorize text |\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r packages, echo=TRUE}\n# Load all packages \nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(textstem)\nlibrary(sentimentr)\nlibrary(topicmodels)\nlibrary(flextable)\nlibrary(knitr)\nlibrary(omni)\n\n```\n:::\n\n::: {.column width=\"40%\"}\n![](images/preprocessing.png)\n:::\n:::::\n\n```{r, include=FALSE}\n# Create a fake, tidy qualitative dataset\ntext_data <- tibble(\n  line = 1:10,\n  speaker = c(\"interviewer\", \"participant\", \"participant\", \"interviewer\", \"participant\",\n              \"participant\", \"interviewer\", \"participant\", \"participant\", \"participant\"),\n  participant_id = c(\"I1\", \"P1\", \"P2\", \"I1\", \"P1\", \"P2\", \"I1\", \"P1\", \"P2\", \"P1\"),\n  question = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 0),\n  section = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3),\n  section_label = c(\"Introduction\", \"Introduction\", \"Introduction\", \"Barriers\", \n                    \"Barriers\", \"Barriers\", \"Recommendations\", \"Recommendations\", \n                    \"Recommendations\", \"Recommendations\"),\n  text = c(\n    \"Can you tell me about your experience accessing services in the county?\",\n    \"Sure, I think one of the biggest challenges is transportation in Travis County. There are many people here who do not own their own cars which makes it difficult to access any type of treatment services. On top of that, the buses are never on time so people have to do a lot of planning ahead of time to get to their appointment.\",\n    \"I agree, especially for people in rural areas. It's hard to get to services.\",\n    \"What kinds of barriers do you see in your community?\",\n    \"Travis county definitely has a lack of awareness about available programs.\",\n    \"Also, the application process is confusing and time-consuming. The health administrative offices used to post the applications at midnight and they would take new clients on a rolling basis, so by the time it was morning all applications would be filled.\",\n    \"What are your recommendations for improving access?\",\n    \"More outreach and education campaigns would help Travis county residents, rather than wasting time on trying to get policy changes.\",\n    \"Simplifying the application process would make a big difference.\",\n    \"Providing transportation vouchers could also improve access.\"\n  ),\n  session = c(rep(\"Education\", 5), rep(\"Public Health\", 5))\n) %>% \n  mutate(text = tolower(text),\n         text = str_replace_all(text, \"[^a-zA-Z0-9\\\\s]\", \"\"))\n\ntext_data <- text_data %>% \n  filter(speaker != \"interviewer\")\n```\n\nPre-processing prepares your qualitative data for analysis by\n**cleaning**, **organizing**, and **standardizing** text. This step\nensures the data is usable for word frequency, sentiment analysis, topic\nmodeling, and other text mining techniques.\n\nYou should always assess the **scope and quality** of your data first:\n\n-   Are the data cleaned (lowercase, removed participant names,\n    punctuation, symbols)? \n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"Check out our function that does all of these data cleaning tasks for you in R! You can also segment your text by question, combine multiple interviews and focus groups into one dataset, and more.\",\n    color = \"periwinkle-600\",\n    fixed_width_px = 800\n)\n```\n<center> \n  <a href=\"https://www.example.com\" target=\"_blank\" rel=\"noopener\" class=\"yellow-btn\">\n    Cleaning transcripts function\n  </a>\n</center>\n-   How much text do you have? (Word counts per document/response.)\n\n-   How many participants contributed?\n\n-   How detailed or shallow are the responses?\n\n-   Whose voices are you analyzing?\n\nOnce you understand your data, you can decide which pre-processing steps\nare appropriate.\n\n## Tokenization {.unnumbered}\n\nTokenization breaks text into individual pieces—usually words, but\nsometimes phrases or sentences.\n\n-   Most text mining techniques require tokenized text.\n\n-   This is common for word frequency analysis, sentiment analysis, and\n    topic modeling.\n\n-   After tokenization, we lose the flow of sentences which is generally\n    okay for some analyses, but not for narrative analysis or content\n    analysis.\n\n```{r}\n# %%%%%%%%%%%%%%% Tokenize text %%%%%%%%%%%%%%%\n\n# Tokenize text\ntokenized_text <- text_data %>% \n  unnest_tokens(word, text)\n\n```\n\n**What are Stop Words?**\n\nStop words are common words like \"the\", \"and\", \"but\" that don’t add much\nmeaning on their own. Because we want to focus on broader themes from\nour participants' voices, we usually want to remove stop words after\ntext is tokenized. This process allows us to focus on content-rich\nwords, like nouns and verbs. We should remove stop words when running a\nword frequency analysis, topic modeling, or sentiment analysis.\n\nWe can also create custom stop words that we want to remove from our\nanalyses, for example, location names or project-specific words that\nappear frequently but aren’t relevant to the analysis.\n\n**Example:**\n\n-   In the example below, my custom stop words are \"travis\" and\n    \"county\", and they're added to the stop_words bank which includes\n    other words like \"but\", \"is\", \"the\", etc. If you'd like to see all\n    of the stop words, see View(stop_words).\n\n```{r}\n# %%%%%%%%%%%%%%% Stop words %%%%%%%%%%%%%%%\n\n# Use standard stop words + custom ones (e.g., counties, names)\nmy_stop_words <- bind_rows(stop_words, \n                           data.frame(word = c(\"travis\", \n                                               \"county\"), lexicon = \"custom\"))\n\ncleaned_text <- tokenized_text %>% \n  anti_join(my_stop_words, by = \"word\")\n\n```\n\n## Stemming {.unnumbered}\n\nStemming words cuts them down to their root forms (e.g., \"running\"\nbecomes \"run\"). This pre-processing step should primarily be used for\ntopic modeling or for other analyses that you decide that exact word\nform isn’t critical. This step is recommended to use with larger\ndatasets, with typically hundreds of documents or more, but for our\npurposes we recommend stemming any time you have enough data for topic\nmodeling (around 5,000 total words).\n\n```{r}\n# %%%%%%%%%%%%%%% Stemming %%%%%%%%%%%%%%%\n\nlibrary(textstem)\n\nstemmed_text <- cleaned_text %>%\n  mutate(stem_word = stem_words(word))\n```\n\n## Lemmatization {.unnumbered}\n\nLemmatization converts words to their dictionary root and keeps the\nwords readable to the user (e.g., \"better\" becomes \"good\"). This\npre-processing step should be used for word frequency analysis,\nsentiment analysis, or thematic/content analysis as it works will with\nsmall and large corpuses.\n\n```{r}\n\n# %%%%%%%%%%%%%%% Lemmatization %%%%%%%%%%%%%%%\n\nlemmatized_text <- cleaned_text %>% \n  mutate(lem_word = lemmatize_words(word))\n\n```\n\nLet's compare the stem words, vs lemmatized words against the original\ntokenized words:\n\n```{r}\ncompare_words <- cleaned_text %>% \n  left_join(stemmed_text, \n            by = join_by(line, \n                         speaker, \n                         participant_id, question, section, section_label,\n                         session, word)) %>% \n  left_join(lemmatized_text,\n            by = join_by(line, \n                         speaker, \n                         participant_id, question, section, section_label,\n                         session, word)) %>%\n  slice(1:10) %>%\n  kable()\n```\n\n```{r, include=TRUE}\ncompare_words\n```\n\n```{r, echo=FALSE}\n# Create a data frame for the table\ncomparison_table <- data.frame(\n  \"Stemming\" = c(\n    \"You need speed over accuracy\",\n    \"Working with large datasets\",\n    \"Basic information retrieval tasks\",\n    \"Search engines or topic modeling where fine detail isn't critical\",\n    \"Not concerned about human readability\"\n  ),\n  \"Lemmatization\" = c(\n    \"Slower but more accurate\",\n    \"Better for small datasets\",\n    \"Better for nuanced analysis\",\n    \"Better for in-depth analysis\",\n    \"Maintains readable words)\"\n  )\n)\n\n# Print the table\nkable(comparison_table,\n      caption = \"When to Use Stemming vs. Lemmatization\",\n      align = 'll')\n\n```\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #F7b925;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Analyses</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-06-Yellow.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n## Word frequency analysis {.unnumbered}\n\n::::: columns\n::: {.column width=\"30%\"}\n![](images/analysis3.png)\n:::\n\n::: {.column width=\"70%\"}\nWord frequency analysis is a simple and effective method that counts how\noften specific words appear in your dataset. It’s often used early in\nqualitative analysis to get a general sense of prominent topics, but it\ncan also be applied as a stand-alone method to identify frequently\nmentioned issues in participant responses. Word frequency works best\nwhen you have a reasonable amount of text to analyze. As a general best\npractice, having at least 100–200 content words (excluding stop words)\nallows for more reliable interpretation of patterns, though smaller\ndatasets can still offer preliminary insights.\n\nTo ensure accuracy, it’s important to pre-process your text. Using\nlemmatized words helps avoid counting variations like “run” and\n“running” separately, and removing stop words—common words such as “the”\nor “and”—allows you to focus on terms that carry more meaning. In cases\nwhere participants mention names of places or projects repeatedly,\ncustom stop word lists can also help refine the results.\n:::\n:::::\n\nWhile word frequency counts themselves do not explain context or\nmeaning, they can help point to emerging themes by highlighting concepts\nthat recur across participants or within particular sections of\ndiscussion. For example, if words like “transportation,” “access,” and\n“barrier” frequently appear in responses about service challenges, they\nsignal a potential theme that warrants deeper exploration. Word\nfrequency analysis can also help guide more interpretive methods such as\nthematic or content analysis, and is most useful when findings are\ncontextualized within the broader dataset and research goals.\n\n```{r}\n# %%%%%%%%%%%%%%% Word frequencies %%%%%%%%%%%%%%%\n# Word counts\nword_counts <- lemmatized_text %>%\n  count(word, sort = TRUE)\n\nhead(word_counts)\n\n# Visualize the word counts\n\nword_counts %>%\n  top_n(5) %>%\n  ggplot(aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Top 10 Most Frequent Words\", x = \"Word\", y = \"Count\") +\n  theme_omni()\n\n```\n\n## Sentiment analysis {.unnumbered}\n\n**Sentiment analysis** is a method that measures the emotional tone of\ntext by categorizing words or phrases as positive, negative, or neutral.\nIt can be used to quickly gauge participants’ attitudes toward a topic\nor to assess the overall tone of responses across interviews, focus\ngroups, or surveys. Sentiment analysis works best when you have larger\namounts of text—ideally, datasets with several hundred words or\nmore—because emotional tone can vary within short responses, making\nsentiment harder to interpret in very small datasets. However, it can\nstill offer insights in smaller datasets when applied carefully and when\nresults are presented as exploratory.\n\nBest practices for sentiment analysis include pre-processing your text\nby removing stop words and standardizing language through lemmatization.\nThis ensures consistent scoring and avoids misclassifications due to\nword variations. Sentiment analysis typically relies on pre-built\nlexicons, such as Bing, AFINN, or the NRC Emotion Lexicon, which assign\nemotional values to words. It’s important to note that these lexicons\nwere often designed for general use (such as social media text) and may\nrequire customization to fit specific public health or social science\ncontexts. For example, in health-related interviews, a word like\n\"treatment\" might appear frequently and carry different sentiment\ndepending on the discussion’s focus.\n\nWhile sentiment analysis doesn’t capture nuance or context in the way\nmanual coding can, it can highlight patterns of emotional tone across\ndatasets or within specific discussion topics. For example, sentiment\nanalysis might reveal that participants express more negative sentiment\nwhen discussing barriers to accessing services, and more positive\nsentiment when discussing recommendations for future improvements. These\ninsights can help guide deeper qualitative coding or serve as an\nadditional layer of analysis to support findings. As with any automated\nmethod, it’s important to review and interpret results in context and to\ndocument any limitations or adaptations made to the analysis.\n\n```{r}\n\nlibrary(tidytext)\n\n# Get sentiment lexicon (if you wanted to test for positive, negative, and 8 emotions, replace \"bing\" with \"nrc\")\nbing <- get_sentiments(\"bing\")\n\n# Join with your text\nsentiment_data <- lemmatized_text %>%\n  inner_join(bing, by = \"word\")\n\n# Count positive vs negative\nsentiment_summary <- sentiment_data %>%\n  count(sentiment)\n\nsentiment_summary\n\n# Visualize\nsentiment_summary %>%\n  ggplot(aes(x = sentiment, y = n, fill = sentiment)) +\n  geom_col() +\n  labs(title = \"Sentiment Analysis\", x = \"Sentiment\", y = \"Word Count\") +\n  scale_fill_omni_discrete() +\n  theme_omni()\n\n```\n\n```{r}\n\n# Filter for the \"Recommendations\" section\nrec_data <- lemmatized_text %>%\n  filter(section_label == \"Recommendations\")\n\n# Join with the Bing sentiment lexicon\nbing_sentiment <- get_sentiments(\"bing\")\n\nrec_data <- rec_data %>%\n  inner_join(bing_sentiment, by = \"word\")\n\n# Calculate sentiment score by line\nsentiment_scores <- rec_data %>%\n  count(line, sentiment) %>%\n  pivot_wider(\n    names_from = sentiment,\n    values_from = n,\n    values_fill = list(n = 0)   # Fill missing sentiment types with 0\n  ) %>%\n  mutate(\n    positive = if_else(is.na(positive), 0L, positive),\n    negative = if_else(is.na(negative), 0L, negative),\n    sentiment_score = positive - negative\n  )\n\n# Join sentiment scores back to the original text\nrec_data_with_scores <- cleaned_text %>%\n  left_join(sentiment_scores, by = \"line\")\n\n```\n\n**Tip for finding quotes by tone**\n\nLet's say you want to pull a quote to include in a report and you know\nthat you want it to be a positively toned quote in a \"recommendations\"\nsection of your discussion. You can use the sentimentr package to gather\nsentiments of each sentence (or segment) and sort by sentiment score to\nfind the quote with the most positive tone in your criteria.\n\nSee an example below:\n\n```{r}\n\nlibrary(sentimentr)\n\ntext_data_wsentiment <- sentiment(text_data$text)\n\nsentiment_summary <- text_data_wsentiment %>%\n  group_by(element_id) %>%\n  summarize(\n    avg_sentiment = mean(sentiment, na.rm = TRUE),\n    sd_sentiment = sd(sentiment, na.rm = TRUE),\n    n_sentences = n()\n  )\n\n# Combine summarized sentiment scores with your original data\ntext_data_with_sentiment <- text_data %>%\n  mutate(element_id = row_number()) %>%\n  left_join(sentiment_summary, by = \"element_id\")\n\n# View the combined data\ntext_data_with_sentiment\n```\n\n**Mixed-methods with sentiment analysis**\n\nBecause sentiment analysis gives you a numeric output as a sentiment\nscore, you can imagine instances where you may want to compare sentiment\nscores between groups, correlate sentiment scores with other variables\nin a quantitative survey, or conduct pre-post comparisons.\n\n## Topic modeling {.unnumbered}\n\nTopic modeling is an automated method used to identify themes or topics\nacross large collections of text. It uses algorithms to group together\nwords that frequently appear in similar contexts, helping reveal hidden\npatterns or structures in qualitative data. Topic modeling is especially\nuseful when you have large datasets—typically a minimum of 5,000 words\nor more spread across multiple documents or participant responses. The\nmethod works best when documents are of relatively similar length, which\nhelps the model assign topics more evenly.\n\nThis method is **not** recommended for use on smaller corpuses and would\nbe considered poor practice to rely on it to generate meaningful\ninsights. If you don't have enough data for this analysis, consider\nusing the content analysis strategy to identify themes in the data.\n\nBefore running a topic model, it’s important to pre-process your text.\nBest practices include stemming words to reduce variation (so that “run”\nand “running” are treated as the same word) and removing stop words to\nfocus on meaningful content. Topic modeling doesn’t require predefined\ncodes or themes, making it a good exploratory tool for surfacing\nunexpected topics in your data. However, it’s important to remember that\nthese topics are generated algorithmically—they group terms based on\nstatistical patterns, not human interpretation. As a result, human\nreview is always needed to interpret and label the topics in a way that\nmakes sense for your project and participants.\n\nTopic modeling can complement manual coding by offering a high-level\nview of common themes, pointing analysts toward areas that may warrant\ndeeper exploration. For example, a topic model might surface clusters of\nwords related to barriers (“transportation,” “access,” “cost”) and\nanother cluster about solutions (“education,” “outreach,” “support”),\ngiving you a starting point for thematic analysis. Commonly used R\npackages for topic modeling include topicmodels, which provides\nalgorithms like Latent Dirichlet Allocation (LDA), and tm for\npre-processing and managing textual data.\n\n```{r}\n\nlibrary(tm)\nlibrary(topicmodels)\n\n# Create Document-Term Matrix (DTM)\ndtm <- stemmed_text %>%\n  count(document = 1, word) %>%\n  cast_dtm(document, word, n)\n\n# Fit LDA models with 2 and 3 topics\nlda_model_2 <- LDA(dtm, k = 2, control = list(seed = 1234))\nlda_model_3 <- LDA(dtm, k = 3, control = list(seed = 1234))\n\n# Compare perplexity (lower is better)\nperplexity_2 <- perplexity(lda_model_2)\nperplexity_3 <- perplexity(lda_model_3)\n\n# Print perplexity values to see how many topics we should go with- in this case 2 topics perform better than 3\nperplexity_2\nperplexity_3\n\n# View top terms for both models\ntopics_2 <- tidy(lda_model_2, matrix = \"beta\")\ntopics_3 <- tidy(lda_model_3, matrix = \"beta\")\n\n# Top terms for 2-topic model\ntop_terms_2 <- topics_2 %>%\n  group_by(topic) %>%\n  top_n(10, beta) %>%\n  arrange(topic, -beta)\n\n# Top terms for 3-topic model\ntop_terms_3 <- topics_3 %>%\n  group_by(topic) %>%\n  top_n(10, beta) %>%\n  arrange(topic, -beta)\n\n# Display the top terms to compare interpretability\ntop_terms_2\ntop_terms_3\n\n```\n\n## Content analysis {.unnumbered}\n\nContent analysis is a method used to count how often predefined\nconcepts, themes, or categories appear in qualitative data. It relies on\na dictionary—a list of key terms or phrases—designed to reflect your\nevaluation questions or coding framework. Content analysis works well\nfor both small and large datasets, making it a flexible tool when you\nwant to systematically measure the presence of specific themes across\ninterviews, focus groups, or open-ended survey responses.\n\nA key requirement for effective content analysis is a carefully designed\ndictionary that accurately captures the concepts you're interested in.\nThis might include terms related to barriers, facilitators, or\nrecommendations, depending on the project’s goals. Best practice is to\nvalidate the dictionary by reviewing examples of matched text to make\nsure the terms are identifying the intended content. You may need to\nrefine the dictionary over time, adding synonyms or removing words that\ngenerate false positives.\n\nContent analysis can help answer questions like \"How frequently do\nparticipants mention prevention strategies?\" or \"What percentage of\nresponses reference funding challenges?\" It is particularly useful when\nyou need to quantify qualitative data for reporting purposes, or when\nyou want to compare how frequently themes appear across different\nstakeholder groups. In R, the quanteda package offers efficient tools\nfor dictionary-based content analysis, allowing you to apply a\ndictionary and quickly summarize how often key terms or concepts appear\nin the dataset.\n\n```{r}\n\nlibrary(quanteda)\n\n# Create the corpus from your text column\ncorp <- corpus(text_data$text)\n\n# Create tokens from the corpus\ntokens_obj <- quanteda::tokens(corp, \n                     what = \"word\", \n                     remove_punct = TRUE, \n                     remove_symbols = TRUE)\n\n# Create the document-feature matrix (dfm) from tokens\ndfm_obj <- dfm(tokens_obj, \n               tolower = TRUE)\n\n# View your dfm\ndfm_obj\n\n# Define dictionary and add themes with keywords here\ndict <- dictionary(list(\n\n  prevention = c(\n    \"prevention\", \"educate\", \"education\", \"awareness\", \"outreach\",\n    \"campaign\", \"community education\", \"school programs\", \"early intervention\",\n    \"public awareness\", \"information\", \"training\", \"workshops\", \n    \"parent education\", \"youth programs\", \"risk reduction\", \"media campaigns\"\n  ),\n  \n  treatment = c(\n    \"treatment\", \"therapy\", \"rehab\", \"rehabilitation\", \"detox\", \"detoxification\",\n    \"MAT\", \"medication-assisted treatment\", \"buprenorphine\", \"methadone\",\n    \"naltrexone\", \"suboxone\", \"clinics\", \"recovery support\", \"residential programs\",\n    \"counseling\", \"peer support\", \"case management\", \"behavioral therapy\",\n    \"inpatient\", \"outpatient\", \"continuum of care\", \"access to care\"\n  ),\n  \n  harm_reduction = c(\n    \"harm reduction\", \"naloxone\", \"narcan\", \"needle exchange\", \"syringe service\",\n    \"safe injection\", \"supervised consumption\", \"overdose prevention\", \n    \"fentanyl test strips\", \"distribution\", \"drug checking\", \"safe supply\", \n    \"wound care\", \"community outreach\", \"safer use education\", \"condom distribution\",\n    \"reduction of risk\", \"public health response\"\n  )\n))\n\n\n# Apply dictionary\ndfm_dict <- dfm_lookup(dfm_obj, dictionary = dict)\n\ndfm_dict \n\n# Convert dictionary to dataframe to join with original data\ndfm_df <- convert(dfm_dict, to = \"data.frame\")\n\n# Add row ID to original text data so we can join on it\ntext_data2 <- text_data %>%\n  mutate(doc_id = paste0(\"text\", row_number()))\n\n# Make sure the doc_id matches what's in the DFM\nhead(dfm_df$doc_id)\n\n# Join the DFM (now a data frame) back to your text data by doc_id\ntext_with_counts <- text_data2 %>%\n  left_join(dfm_df, by = \"doc_id\")\n\n# View the combined data\nhead(text_with_counts)\n\n```\n\n## Thematic Analysis in Dedoose\n\n@qualBPT help here!! Can we add anything re coding to this guide in this section, maybe Ivonne's work? What other programs should we add?\n\nIf using Dedoose to conduct thematic analysis, we recommend a\nstructured, reflective approach grounded in reflexive thematic analysis\n(Clarke & Braun, 2006) and informed by critical realism.\n\nHere’s how to approach it:\n\n1.  Start with Familiarization Read through transcripts or responses in\n    full before coding.\n\nUse memos to reflect on initial impressions, patterns, and researcher\nassumptions.\n\n2.  Develop Codes Iteratively Begin with inductive coding (bottom-up),\n    focusing on what participants actually say.\n\nAvoid pre-loading the codebook unless doing deductive analysis is needed\nfor evaluation purposes.\n\nUse Dedoose’s “Descriptor” fields to track context (e.g., stakeholder\ngroup, session type) for later analysis.\n\n3.  Apply Codes Reflexively Apply codes carefully, updating definitions\n    and merging/splitting codes as needed.\n\nEncourage coders to discuss disagreements and maintain an audit trail of\nkey coding decisions.\n\n4.  Construct Themes Thoughtfully After coding, review excerpts by code\n    and group them into broader themes that reflect patterns across\n    participants.\n\nUse Dedoose’s “Code Co-Occurrence” and “Code Application” tools to\nexplore theme structure.\n\n5.  Acknowledge Researcher Role Reflect on how your perspective,\n    language, and interpretation influence theme development.\n\nBe cautious not to overstate findings; describe patterns and nuance\nrather than asserting consensus.\n\n6.  Document Everything Keep a record of how codes and themes evolved.\n\nClearly state the number of participants or excerpts that informed each\ntheme.\n\nMake your interpretive lens and limitations explicit—especially when\ndatasets are small or uneven across groups.\n\n> Thematic analysis in Dedoose should still reflect Omni’s commitment to\n> critical realism: treat participant input as reflecting real issues\n> shaped by context and perspective, and be transparent about what the\n> data can—and cannot—support.\n\n## Combining analyses {.unnumbered}\n\nNo single method can fully capture the richness and complexity of\nqualitative data. At Omni, we often combine different qualitative\nanalysis approaches to explore various angles of our data and answer\nnuanced evaluation questions. Pairing methods like word frequency,\nsentiment analysis, content analysis, and thematic analysis can help\nreveal both patterns and meaning, ensuring our findings are grounded in\nevidence and provide actionable insights.\n\n","srcMarkdownNoYaml":"\n\n```{r, echo=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)\n```\n\n```{r, echo=FALSE}\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(omni)\nlibrary(scales)\n```\n\n```{=html}\n<style>\n  .outer {\n    display: block;\n    width: 100%;\n  }\n\n  .green-box {\n    --box-h: 80px;                     /* height of the bar */\n    --bar-bg: #41816F;                 /* default background color */\n    --bar-text-color: #ffffff;         /* default text color */\n    --pattern-img: url(\"images/Pattern-01-Teal.png\");\n\n    height: var(--box-h);\n    width: 100%;\n    display: flex;\n    background-color: var(--bar-bg);\n    color: var(--bar-text-color);\n    border-radius: 0;\n    overflow: visible;\n  }\n\n  .green-col {\n    flex: 1 1 auto;\n    display: flex;\n    align-items: center;\n    padding-left: 1em;\n    font-size: 3.4rem;\n    font-weight: 300;\n    color: var(--bar-text-color);\n    white-space: nowrap;\n  }\n\n  .green-col.pattern {\n    flex: 0 0 auto;\n    padding: 0 !important;\n    display: flex;\n    align-items: center;\n    justify-content: flex-end;\n    background: none;\n  }\n\n  .green-col.pattern img {\n    height: 100%;\n    width: auto;\n    display: block;\n  }\n</style>\n```\n\n```{=html}\n<style>\n.yellow-btn {\n  display: inline-block;\n  background-color: #fcd82b;\n  color: #000000;\n  padding: 0.5em 1.2em;\n  text-decoration: none;\n  border: none;\n  border-radius: 0px;   /* squared edges? change to 0 */\n  font-weight: bold;\n  font-size: 1.5rem;\n}\n.yellow-btn:hover {\n  background-color: #e6c323; /* slightly darker on hover */\n}\n</style>\n```\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #41816F;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Executive Summary</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-01-Teal.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<div style=\"background-color: #c5dfd9; padding: 20px; border-radius: 0px;\">\n  <p>Omni's qualitative analysis guide supports staff in conducting transparent, systematic, and actionable qualitative research in complex, real-world settings. Our clients often require timely, credible findings grounded in participants' lived experiences. To meet these demands, this guide outlines practical workflows rooted in two complementary paradigms: **[pragmatism]{.teal-600}** and **[critical realism]{.teal-600}**.</p>\n\n  <ul>\n    <li>**[Pragmatism]{.teal-600}** directs us to focus on real-world utility and actionable insights.</li>\n    <li>**[Critical realism]{.teal-600}** encourages us to understand participant perspectives while acknowledging how those perspectives are shaped by broader structures and contexts.</li>\n  </ul>\n\n  <p>Together, these paradigms support our commitment to meaningful, context-aware findings that are both usable and methodologically sound.</p>\n\n  <p>This guide provides step-by-step instructions for conducting qualitative analysis, from data pre-processing to reporting. It includes techniques like word frequency analysis, sentiment analysis, thematic coding, content analysis, and topic modeling, all implemented with transparency and adaptability in mind.</p>\n\n</div>\n\n### Key Features of the Guide:\n\n```{r, echo=FALSE}\nwidth_px <- 40\ntext_width <- 800\nicon_color_fg <- \"white\"\nicon_color_bg <- 'teal-400'\n\nicon_text_grid(\n    omni_icon('education', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        text = '<strong>Tool Flexibility:</strong> While R is our preferred tool for reproducibility and integration with quantitative methods, the guide also includes best-practice workflows for using Dedoose (for team-based coding) and AI tools like NotebookLM (for early exploratory work).',\n        text_width\n    ),\n    omni_icon('security', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        '<strong>Method Selection Guidance:</strong> We offer practical decision points based on dataset size, analytic goals, and project context, ensuring methods match the scope and purpose of each evaluation.',\n        text_width\n    ),\n    omni_icon('vault', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        '<strong>Structured Workflows:</strong> Omni workflows help analysts navigate all stages of qualitative analysis—from clarifying analytic frameworks and coding approaches to integrating qualitative and quantitative data in mixed-methods designs.',\n        text_width\n    ),\n    omni_icon('vault', width_px, icon_color_bg, icon_color_fg),\n    icon_text(\n        '<strong>Adaptability:</strong> Whether you\\'re working with rich interview transcripts or brief open-ended survey responses, the guide provides adaptable tools that emphasize rigor, transparency, and context.',\n        text_width\n    ),\n    column_gap_px = 10, # default is 10\n    row_gap_px = 10, # default is 10\n    width_px = width_px\n)\n```\n\n<br>\n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"By centering participant voices, documenting our methods clearly, and maintaining a reflexive stance, this guide strengthens the credibility and usefulness of Omni's qualitative work. It equips staff with tools and strategies that not only meet evaluation standards but also foster accountability, responsiveness, and connection to the communities we serve.\",\n    color = \"teal-600\",\n    fixed_width_px = 800\n)\n```\n\n<br>\n\n<details>\n<summary><strong>Version Control</strong></summary>\n\n| Version | Description              | Date released |\n|---------|--------------------------|---------------|\n| -4.0    | Adding interactive features   | 2025-08-14    |\n| -5.0    | Second draft for review and rebrand   | 2025-08-14    |\n| -6.0    | Initial draft for review  | 2025-03-26    |\n\n</details>\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #C65A8B;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Introduction</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-02-Plum.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n::::: columns\n::: {.column width=\"80%\"}\nAt Omni, we conduct qualitative research in complex, real-world\nenvironments. Our clients and projects often require fast timelines,\nnuanced insights, and transparent methods that can stand up to external\nreview. To meet these needs, we use systematic, reproducible qualitative\nanalysis methods grounded in **[pragmatism]{.plum-600}** and **[critical realism]{.plum-600}**.\n:::\n\n::: {.column width=\"20%\"}\n![](images/analysis1.png)\n:::\n:::::\n\nThis approach ensures our findings are actionable and reflect the\nrealities and lived experiences of participants, while acknowledging the\ninfluence of context, interpretation, and limitations in our data.\n\n## Purpose {.unnumbered}\n\nThis guide was developed to support Omni staff in conducting\nmethodologically sound, transparent, and reproducible qualitative\nanalyses. It offers practical, step-by-step instructions and highlights\nbest practices for text pre-processing, analysis, and reporting. Whether\nworking with interview transcripts, focus groups, open-ended survey\nresponses, or other text-based data, Omni teams can use this guide to\nproduce findings that are both grounded in participants’ voices and\nuseful for program improvement and decision-making.\n\n#### How does this guide align with Omni's core values?\n\n```{=html}\n<style>\n  .values-row {\n    --card-size: 180px;        /* card width/height */\n    --plum: #921c4c;\n    --front-text: #ffffff;\n    --back-text: #2b2b2b;\n    display: grid;\n    grid-template-columns: repeat(4, var(--card-size));\n    gap: 16px;\n    justify-content: start;\n    align-items: start;\n  }\n\n  .card {\n    width: var(--card-size);\n    height: var(--card-size);\n    perspective: 1000px;\n    cursor: pointer;\n    border: none;\n    border-radius: 0;\n    padding: 0;\n    background: none;\n    outline: none; /* remove black outline entirely */\n  }\n\n  .card-inner {\n    position: relative;\n    width: 100%;\n    height: 100%;\n    transition: transform 420ms ease;\n    transform-style: preserve-3d;\n  }\n\n  .card:hover .card-inner,\n  .card.is-flipped .card-inner {\n    transform: rotateY(180deg);\n  }\n\n  .card-face {\n    position: absolute;\n    inset: 0;\n    display: grid;\n    place-items: center;\n    padding: 14px;\n    border-radius: 0;\n    backface-visibility: hidden;\n    text-align: center;\n  }\n\n  /* Bigger front font */\n  .card-front {\n    background: var(--plum);\n    color: var(--front-text);\n    font-weight: 700;\n    font-size: 1.8rem; /* larger than before */\n    letter-spacing: .3px;\n  }\n\n  /* Bigger back font */\n  .card-back {\n    background: #f5f3f2;\n    color: var(--back-text);\n    transform: rotateY(180deg);\n    font-size: 1.5rem; /* larger text */\n    line-height: 1.35;\n  }\n</style>\n\n<div class=\"values-row\">\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Inquiry</div>\n      <div class=\"card-face card-back\">\n        Provides clear, rigorous methods that encourage thoughtful exploration of participants’ experiences.\n      </div>\n    </div>\n  </button>\n\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Agility</div>\n      <div class=\"card-face card-back\">\n        Equips staff with adaptable tools to meet diverse project needs and respond efficiently to change.\n      </div>\n    </div>\n  </button>\n\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Connection</div>\n      <div class=\"card-face card-back\">\n        Centers participants’ voices and strengthens relationships with the communities and partners we serve.\n      </div>\n    </div>\n  </button>\n\n  <button class=\"card\">\n    <div class=\"card-inner\">\n      <div class=\"card-face card-front\">Accountability</div>\n      <div class=\"card-face card-back\">\n        Uses transparent, reproducible analysis practices to keep our work credible, ethical, and trustworthy.\n      </div>\n    </div>\n  </button>\n</div>\n\n<script>\n  document.querySelectorAll('.card').forEach(card => {\n    card.addEventListener('click', () => card.classList.toggle('is-flipped'));\n    card.addEventListener('keydown', (e) => {\n      if (e.key === 'Enter' || e.key === ' ') {\n        e.preventDefault();\n        card.classList.toggle('is-flipped');\n      }\n    });\n  });\n</script>\n```\n\n## What This Guide Covers {.unnumbered}\n\nYou’ll find guidance on:\n\n-   **[Text mining / Natural Language Processing (NLP)]{.plum-600}**\\\n    (e.g., word frequency, sentiment analysis, topic modeling)\n\n-   **[Thematic and content analysis]{.plum-600}**\\\n    (e.g., dictionary-based coding, thematic coding frameworks)\n\n-   **[Narrative analysis @qualBPT do we need?]{.plum-600}**\\\n    (Optional: e.g., structural analysis, language style matching)\n\n-   **[Visualizing and reporting qualitative data]{.plum-600}**\\\n    (e.g., word clouds, bar charts, heatmaps, joint displays for\n    mixed-methods)\n\nThese methods can be applied to text from: Word documents (.docx), PDFs\n(.pdf), Zoom transcripts (.docx or .txt), open-ended survey responses\n(.xlsx or .csv), and more.\n\nIn many applied settings, qualitative methods (and especially\nmixed-methods) are described in vague terms. Reports may mention \"themes\nemerging\" or \"triangulating findings,\" but rarely explain the actual\nprocess used to get there. This vagueness makes it hard to replicate or\nassess qualitative findings and limits their credibility.\n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"At Omni, we aim to avoid vague or ad hoc practices. We document clear, systematic workflows that integrate qualitative insights with\nquantitative data when appropriate.\",\n    color = \"plum-600\",\nfixed_width_px = 800\n)\n```\n\n### Why Are Existing Descriptions of Methods So Vague? {.unnumbered}\n\nSeveral factors explain the common lack of clarity in methodology:\n\n-   Different disciplines (e.g., public health vs. education research)\n    use different frameworks, leading to inconsistent approaches.\n\n-   Quantitative research has standard tools (R, SPSS, Stata), but\n    qualitative tools (NVivo, MAXQDA, Dedoose) often rely on manual\n    processes and don't always integrate cleanly with quantitative\n    workflows.\n\n-   Qualitative analysis requires human interpretation and reflexivity.\n    Documenting every step is time-consuming, and under tight timelines,\n    many organizations skip this critical process.\n\n-   Qualitative and quantitative teams often work separately, without\n    shared standards or mixed-methods integration.\n\n[Hannah is just messing around here to see how an .mp4 reads into R and plays]\n```{=html}\n<video width=\"640\" height=\"360\" controls>\n  <source src=\"videos/test Video Project 1.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n```\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #89A046;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Omni's Approach to Analysis</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-03-Olive.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\nOmni’s qualitative practice is grounded in **[pragmatism]{.olive-green-600}** and **[critical realism]{.olive-green-600}**.\n\n-   **[Pragmatism]{.olive-green-600}** means we focus on providing useful, actionable insights for real-world decision-making. Our goal is to help clients\n    understand participants’ experiences in ways that inform policy,\n    programs, and practice. This reflects a pragmatic stance rooted in\n    the work of scholars like <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/2345678906292462\" target=\"_blank\" rel=\"noopener\">\n      Morgan (2007)\n    </a>\n    and <a href=\"https://www.taylorfrancis.com/books/mono/10.4324/9781351017831/explaining-society-berth-danermark-mats-ekstr%C3%B6m-jan-ch-karlsson\" target=\"_blank\" rel=\"noopener\">\n      Danermark et al. (2015)\n    </a>, emphasizing inquiry that is guided by consequences and utility.\n\n-   **[Critical realism]{.olive-green-600}** acknowledges that while participants’ experiences reflect real phenomena (barriers, challenges,\n    successes), they are shaped by context and perspective. Influenced\n    by <a href=\"https://www.tandfonline.com/doi/full/10.1080/13645579.2016.1144401\" target=\"_blank\" rel=\"noopener\">\n      Fletcher’s (2017)\n    </a> work and work by others, this perspective guides us to acknowledge\n    our limitations, reflect on researcher influence, and avoid\n    overstating claims.\n\n-   Omni’s qualitative practice draws on reflexive thematic analysis\n  <a href=\"https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa\" target=\"_blank\" rel=\"noopener\">\n    (Clarke &amp; Braun, 2006)\n  </a>\n  and content analysis\n  <a href=\"https://journals.sagepub.com/doi/abs/10.1177/1049732305276687\" target=\"_blank\" rel=\"noopener\">\n    (Hseih &amp; Shannon, 2005)\n  </a>,\n  emphasizing the researcher’s active role in interpreting data and\n  constructing themes, in alignment with our commitment to critical\n  realism and methodological transparency.\n\n### Transparent and Systematic Workflows {.unnumbered}\n\nAt Omni, we use structured qualitative and mixed-methods workflows.\nThese processes are transparent, systematic, and designed to integrate\nqualitative and quantitative findings when appropriate.\n\n![](images/workflow_green.png)\n\n**Example Omni Workflow for Qualitative analysis:**\n\n1.  Clarify Purpose and Analytic Framework\n\n  -  Define the evaluation questions and purpose of the qualitative\n    analysis\n\n  -  Select an analytic method\n\n2.  Prepare and Familiarize with the Data\n\n  -  Clean transcripts/text\n\n  -  Read through interviews, discussions, or sessions at least once\n    (text like reports you may not need context for, use your\n    discretion)\n\n3.  Systematic Coding of the Data\n\n  -  Deductive, Inductive, or hybrid\n\n  -  Deductive coding uses predefined codes (like our dictionaries) to\n    apply to our data\n\n  -  Inductive coding uses text mining or topic modeling to identify\n    emergent themes\n\n4.  Theme Development and Refinement\n\n  -  Analyze and refine codes/themes\n\n5.  Interpretation and Synthesis\n\n  -  Compare themes to evaluation questions and contextual frameworks\n\n6.  Explore themes visually\n\n7.  Get ready for reporting\n\n## Choosing Your Analysis Tool: R vs. Dedoose vs. AI {.unnumbered}\n\n```{=html}\n<style>\n  /* ---- Carousel theme ---- */\n  .omni-carousel {\n    --bg: #89A046;          /* slide background */\n    --title: #ffffff;       /* slide title color */\n    --body: #000000;        /* body text color */\n    --accent: #2b2b2b;      /* arrows/dots */\n    position: relative;\n    max-width: 900px;       /* set a fixed width if you like */\n    margin: 12px auto 24px;\n    overflow: hidden;\n    border: 0;              /* squared edges */\n  }\n\n  .oc-track {\n    display: grid;\n    grid-auto-flow: column;\n    grid-auto-columns: 100%;\n    transition: transform 380ms ease;\n  }\n\n  .oc-slide {\n    background: var(--bg);\n    color: var(--body);\n    min-height: 280px;\n    display: grid;\n    padding: 20px 22px;\n    align-content: start;\n    gap: 10px;\n  }\n\n  .oc-title {\n    color: var(--title);\n    font-size: 1.6rem;\n    font-weight: 800;\n    letter-spacing: .2px;\n    margin: 0 0 2px 0;\n  }\n\n  .oc-cols {\n    display: grid;\n    grid-template-columns: 1fr 1fr;\n    gap: 16px;\n  }\n\n  .oc-tag {\n    display: inline-block;\n    font-weight: 700;\n    padding: 2px 8px;\n    background: rgba(255,255,255,0.75);\n    border: 0;\n  }\n\n  .oc-list {\n    margin: 6px 0 0 0;\n    padding-left: 18px;\n  }\n  .oc-list li { margin: 4px 0; }\n\n  /* Arrows */\n  .oc-arrow {\n    position: absolute;\n    top: 50%;\n    translate: 0 -50%;\n    background: #fff;\n    color: var(--accent);\n    border: 2px solid var(--accent);\n    padding: 6px 10px;\n    cursor: pointer;\n    font-weight: 800;\n  }\n  .oc-prev { left: 6px; }\n  .oc-next { right: 6px; }\n\n  /* Dots */\n  .oc-dots {\n    display: flex;\n    gap: 6px;\n    justify-content: center;\n    margin-top: 8px;\n  }\n  .oc-dot {\n    width: 10px; height: 10px; border-radius: 50%;\n    background: #d1d5db; border: 0; cursor: pointer;\n  }\n  .oc-dot[aria-current=\"true\"] { background: var(--accent); }\n\n  /* Responsive */\n  @media (max-width: 720px) {\n    .oc-cols { grid-template-columns: 1fr; }\n    .oc-title { font-size: 1.35rem; }\n  }\n</style>\n\n<div class=\"omni-carousel\" role=\"region\" aria-label=\"Ways of analyzing qualitative data\">\n  <div class=\"oc-track\" data-index=\"0\" aria-live=\"polite\">\n\n    <!-- Slide: R -->\n    <section class=\"oc-slide\" data-name=\"R\">\n      <h3 class=\"oc-title\">R</h3>\n      <div class=\"oc-cols\">\n        <div>\n          <span class=\"oc-tag\">Best when</span>\n          <ul class=\"oc-list\">\n            <li>Datasets are large or mixed (qual + quant), need audit trails.</li>\n            <li>You want reproducibility, automation, version control.</li>\n            <li>Time crunch with existing scripts/templates; IRR optional or phased.</li>\n          </ul>\n        </div>\n        <div>\n          <span class=\"oc-tag\">Pros</span>\n          <ul class=\"oc-list\">\n            <li>Transparent, reproducible code; scalable pipelines.</li>\n            <li>Flexible: word freq, sentiment, dictionaries, topic models.</li>\n            <li>Precise visuals; integrates surveys/demographics easily.</li>\n          </ul>\n          <span class=\"oc-tag\" style=\"margin-top:8px;\">Cons</span>\n          <ul class=\"oc-list\">\n            <li>Setup/learning curve.</li>\n            <li>Requires clear documentation to ensure shared understanding.</li>\n          </ul>\n        </div>\n      </div>\n    </section>\n\n    <!-- Slide: Dedoose -->\n    <section class=\"oc-slide\" data-name=\"Dedoose\">\n      <h3 class=\"oc-title\">Dedoose</h3>\n      <div class=\"oc-cols\">\n        <div>\n          <span class=\"oc-tag\">Best when</span>\n          <ul class=\"oc-list\">\n            <li>Collaborative coding with a team; training new coders.</li>\n            <li>Small–moderate corpora.</li>\n            <li>Quick turnarounds with IRR workflows built in the tool.</li>\n          </ul>\n        </div>\n        <div>\n          <span class=\"oc-tag\">Pros</span>\n          <ul class=\"oc-list\">\n            <li>Multi‑user collaboration, role permissions, IRR features.</li>\n            <li>Low barrier for non‑programmers.</li>\n          </ul>\n          <span class=\"oc-tag\" style=\"margin-top:8px;\">Cons</span>\n          <ul class=\"oc-list\">\n            <li>License cost.</li>\n            <li>Automation/custom analyses limited vs. code.</li>\n            <li>Reproducibility/export trails less granular than scripted pipelines.</li>\n          </ul>\n        </div>\n      </div>\n    </section>\n\n    <!-- Slide: AI -->\n    <section class=\"oc-slide\" data-name=\"AI\">\n      <h3 class=\"oc-title\">AI</h3>\n      <div class=\"oc-cols\">\n        <div>\n          <span class=\"oc-tag\">Best when</span>\n          <ul class=\"oc-list\">\n            <li>Early exploration: surfacing possible themes/codes quickly.</li>\n            <li>Short corpora; scoping questions; drafting a codebook.</li>\n            <li>Stakeholder previews before deeper human analysis.</li>\n          </ul>\n        </div>\n        <div>\n          <span class=\"oc-tag\">Pros</span>\n          <ul class=\"oc-list\">\n            <li>Very fast; low barrier; helps brainstorm structures.</li>\n            <li>Can summarize, cluster, and compare at a glance.</li>\n            <li>Useful for iteration before moving to R or Dedoose.</li>\n          </ul>\n          <span class=\"oc-tag\" style=\"margin-top:8px;\">Cons</span>\n          <ul class=\"oc-list\">\n            <li>Not a substitute for human interpretation; can hallucinate.</li>\n            <li>Inconsistent across runs; shallow context if prompts are weak.</li>\n            <li>Data governance/privacy limits—avoid sensitive uploads without approvals.</li>\n          </ul>\n        </div>\n      </div>\n    </section>\n\n  </div>\n\n  <!-- Controls -->\n  <button class=\"oc-arrow oc-prev\" aria-label=\"Previous slide\">‹</button>\n  <button class=\"oc-arrow oc-next\" aria-label=\"Next slide\">›</button>\n\n  <!-- Dots -->\n  <div class=\"oc-dots\" role=\"tablist\" aria-label=\"Carousel navigation\">\n    <button class=\"oc-dot\" aria-label=\"R\" aria-current=\"true\"></button>\n    <button class=\"oc-dot\" aria-label=\"Dedoose\"></button>\n    <button class=\"oc-dot\" aria-label=\"AI\"></button>\n  </div>\n</div>\n\n<script>\n(function(){\n  const track = document.querySelector('.oc-track');\n  const slides = Array.from(document.querySelectorAll('.oc-slide'));\n  const prev = document.querySelector('.oc-prev');\n  const next = document.querySelector('.oc-next');\n  const dots = Array.from(document.querySelectorAll('.oc-dot'));\n  let idx = 0, n = slides.length;\n\n  function go(i){\n    idx = (i + n) % n;\n    track.style.transform = `translateX(-${idx*100}%)`;\n    dots.forEach((d, j)=>d.setAttribute('aria-current', j===idx));\n  }\n\n  prev.addEventListener('click', ()=>go(idx-1));\n  next.addEventListener('click', ()=>go(idx+1));\n  dots.forEach((d, j)=>d.addEventListener('click', ()=>go(j)));\n\n  // Keyboard\n  document.querySelector('.omni-carousel').addEventListener('keydown', (e)=>{\n    if(e.key==='ArrowRight') go(idx+1);\n    if(e.key==='ArrowLeft')  go(idx-1);\n  });\n\n  // Touch swipe\n  let startX=null;\n  track.addEventListener('touchstart', e=>{ startX = e.touches[0].clientX; }, {passive:true});\n  track.addEventListener('touchmove', e=>{\n    if(startX===null) return;\n    const dx = e.touches[0].clientX - startX;\n    if(Math.abs(dx)>40){ go(idx + (dx<0 ? 1 : -1)); startX=null; }\n  }, {passive:true});\n})();\n</script>\n\n```\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #CC4100;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Method Selection</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-04-OrangeRed.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n::::: columns\n::: {.column width=\"80%\"}\nOnce pre-processed, your data is ready for analysis. Selecting the\n**[right method]{.orange-red-600}** depends on:\n\n-   How much data you have\n\n-   Your research question(s)\n\n-   Whether you need exploratory insights or answers to specific\n    questions\n\n-   Your integrative framework (if conducting mixed-methods)\n:::\n\n::: {.column width=\"20%\"}\n![](images/data.png)\n:::\n:::::\n\n### Understanding Your Data Before Analysis {.unnumbered}\n\nBefore starting pre-processing or analysis, it’s important to assess the\nscope and quality of your data. Take time to understand:\n\n-   How many participants are included?\n\n-   How much text do you have (word count, number of responses)?\n\n-   How detailed are the responses (in-depth vs. brief)?\n\n-   How are participants grouped (by stakeholder type, session, etc.)?\n\n-   Whose voices are most important to elevate in this analysis?\n\nThis step ensures you choose methods that fit your dataset and align\nwith your project goals. For example:\n\n-   Smaller datasets (under \\~2,000 words) are well-suited to word\n    frequency, sentiment analysis, or basic content analysis.\n\n-   Larger datasets (5,000+ words) can support topic modeling or\n    advanced thematic analysis.\n\nAssessing your data upfront helps set realistic expectations for\nanalysis and ensures your findings are grounded, transparent, and\ndefensible.\n\n### Working with a Small Corpus {.unnumbered}\n\nAt Omni, qualitative research often happens in real-world settings, with\ntight timelines and limited participant availability. We may aim for 10\ninterviews and complete 5. We may expect long, detailed conversations\nand instead get brief answers. Even with these constraints, small\ndatasets can still provide meaningful insights, especially when there is\nconsistency across participant experiences.\n\nWhen working with limited data:\n\n-   Focus on what participants actually shared, rather than attempting\n    to generalize.\n\n-   Document patterns and recurring concerns, while linking them clearly\n    to the project’s research questions.\n\n-   Be transparent about the number of participants, the methods used,\n    and any limitations in interpretation.\n\nSmaller samples can still highlight critical issues—such as barriers to\naccess or common recommendations for program improvement—but the scope\nand representativeness of these findings should be clearly communicated.\n\n### Suggested Methods by Dataset Size and Purpose {.unnumbered}\n\n<!-- | **Analysis** | **Typical Recommendation** | **Wiggle Room with Word Count** | **Why** | -->\n<!-- |-----------------|-----------------|-----------------|---------------------| -->\n<!-- | **Word Frequency** | 500+ words | ✅ High | Works even on **small datasets** for exploratory insights. You can analyze **under 500 words**, but findings may feel anecdotal rather than robust. Best if presented as observations rather than firm conclusions. | -->\n<!-- | **Sentiment Analysis** | 1,000+ words | ✅ Moderate | Sentiment varies widely within small datasets. Under **1,000 words**, **tone shifts** can skew results. You *can* use it in smaller datasets (500+ words), but you'll need **careful interpretation** and clear disclaimers. | -->\n<!-- | **Content Analysis (Dictionary-based) or Thematic Analysis** | 1,000+ words (small dictionaries) | ✅ Moderate | Works **well with small datasets**, especially if using **focused dictionaries** with clearly defined terms. Fewer words mean fewer opportunities for themes to emerge, so **results need contextualization**. | -->\n<!-- | **Topic Modeling** | 5,000+ words | ❌ Low | Topic modeling relies on **statistical probability**. Anything under **5,000 words** will **struggle to produce meaningful, stable topics**. While you *can* run a topic model on smaller datasets (e.g., 2,000 words), the **results are less trustworthy**, prone to overfitting, and often **hard to interpret**. Not recommended without strong caveats. | -->\n```{r echo=FALSE}\nlibrary(DT)\n\n# Your table data\ndf <- data.frame(\n  Analysis = c(\"Word Frequency\", \"Sentiment Analysis\", \n               \"Content Analysis (Dictionary-based) or Thematic Analysis\", \n               \"Topic Modeling\"),\n  Typical_Recommendation = c(\"500+ words\", \"1,000+ words\", \n                              \"1,000+ words (small dictionaries)\", \n                              \"5,000+ words\"),\n  Wiggle_Room_with_Word_Count = c(\"✅ High\", \"✅ Moderate\", \"✅ Moderate\", \"❌ Low\"),\n  Why = c(\n    \"Works even on small datasets for exploratory insights. You can analyze under 500 words, but findings may feel anecdotal rather than robust. Best if presented as observations rather than firm conclusions.\",\n    \"Sentiment varies widely within small datasets. Under 1,000 words, tone shifts can skew results. You can use it in smaller datasets (500+ words), but you'll need careful interpretation and clear disclaimers.\",\n    \"Works well with small datasets, especially if using focused dictionaries with clearly defined terms. Fewer words mean fewer opportunities for themes to emerge, so results need contextualization.\",\n    \"Topic modeling relies on statistical probability. Anything under 5,000 words will struggle to produce meaningful, stable topics. While you can run a topic model on smaller datasets (e.g., 2,000 words), the results are less trustworthy, prone to overfitting, and often hard to interpret. Not recommended without strong caveats.\"\n  ),\n  stringsAsFactors = FALSE\n)\n\ndatatable(\n  df,\n  escape = FALSE,  # lets you keep emoji and HTML formatting\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE\n  )\n)\n\n```\n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"<highlight>Note:</highlight> If you are working with open-ended responses from a survey\nand have data from <higlight>30 or fewer participants</higlight>, carefully consider\nwhether the data are sufficient for meaningful analysis. In many\ncases, it is appropriate to report that <higlight>there were too few responses to analyze systematically</higlight>. However, if you need to report findings, be transparent about limitations. Focus on <higlight>describing frequently mentioned key words</higlight>, rather than inferring broader themes. Avoid suggesting consensus when data are limited, and clearly state that findings reflect input from a <higlight>small number of participants</higlight>. Transparent reporting maintains the credibility of your analysis.\",\n    color = \"orange-red-600\",\n    fixed_width_px = 800\n)\n```\n\n### Grouping Voices to Guide Analysis {.unnumbered}\n\nBefore you start coding or analyzing, decide:\n\n-   Whose voices are we centering in this analysis?\n\n-   How will we group participant responses?\n\nYour grouping choices influence:\n\n-   Which perspectives are highlighted\n\n-   How themes emerge\n\n-   What questions you can answer\n\nAt Omni, we might group data by:\n\n-   Participant (individual experiences)\n\n-   Stakeholder Group (e.g., educators vs. public health professionals)\n\n-   Discussion Section (e.g., barriers vs. recommendations)\n\nThese decisions should align with the project’s goals and be clearly\ndocumented in reports and presentations.\n\n### Ready for Pre-processing? {.unnumbered}\n\nOnce you understand your data, you’re ready to start **pre-processing**!\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #5776b2;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Cleaning and Pre-Processing Your Data</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-05-Periwinkle.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n## Terminology {.unnumbered}\n\nBefore starting your pre-processing and analysis, it’s important to\nunderstand a few core terms. These concepts are essential for working\nwith text data and deciding on the appropriate pre-processing and\nanalysis steps.\n\n| **Term** | **Definition** |\n|-----------------------|-------------------------------------------------|\n| Tokenization | Breaking text into units (words, phrases, sentences) |\n| Stemming | Reducing words to their root (e.g., \"running\" → \"run\") |\n| Lemmatization | Reducing to dictionary form (e.g., \"better\" → \"good\") |\n| Stop Words | Common words often removed (e.g., \"the\", \"and\") |\n| Document-Term Matrix | Table of word frequencies across documents |\n| TF-IDF | Term importance based on frequency and inverse document frequency |\n| Corpus | Collection of text documents as one dataset |\n| Dictionary | List of keywords/phrases used to code or categorize text |\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r packages, echo=TRUE}\n# Load all packages \nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(textstem)\nlibrary(sentimentr)\nlibrary(topicmodels)\nlibrary(flextable)\nlibrary(knitr)\nlibrary(omni)\n\n```\n:::\n\n::: {.column width=\"40%\"}\n![](images/preprocessing.png)\n:::\n:::::\n\n```{r, include=FALSE}\n# Create a fake, tidy qualitative dataset\ntext_data <- tibble(\n  line = 1:10,\n  speaker = c(\"interviewer\", \"participant\", \"participant\", \"interviewer\", \"participant\",\n              \"participant\", \"interviewer\", \"participant\", \"participant\", \"participant\"),\n  participant_id = c(\"I1\", \"P1\", \"P2\", \"I1\", \"P1\", \"P2\", \"I1\", \"P1\", \"P2\", \"P1\"),\n  question = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 0),\n  section = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3),\n  section_label = c(\"Introduction\", \"Introduction\", \"Introduction\", \"Barriers\", \n                    \"Barriers\", \"Barriers\", \"Recommendations\", \"Recommendations\", \n                    \"Recommendations\", \"Recommendations\"),\n  text = c(\n    \"Can you tell me about your experience accessing services in the county?\",\n    \"Sure, I think one of the biggest challenges is transportation in Travis County. There are many people here who do not own their own cars which makes it difficult to access any type of treatment services. On top of that, the buses are never on time so people have to do a lot of planning ahead of time to get to their appointment.\",\n    \"I agree, especially for people in rural areas. It's hard to get to services.\",\n    \"What kinds of barriers do you see in your community?\",\n    \"Travis county definitely has a lack of awareness about available programs.\",\n    \"Also, the application process is confusing and time-consuming. The health administrative offices used to post the applications at midnight and they would take new clients on a rolling basis, so by the time it was morning all applications would be filled.\",\n    \"What are your recommendations for improving access?\",\n    \"More outreach and education campaigns would help Travis county residents, rather than wasting time on trying to get policy changes.\",\n    \"Simplifying the application process would make a big difference.\",\n    \"Providing transportation vouchers could also improve access.\"\n  ),\n  session = c(rep(\"Education\", 5), rep(\"Public Health\", 5))\n) %>% \n  mutate(text = tolower(text),\n         text = str_replace_all(text, \"[^a-zA-Z0-9\\\\s]\", \"\"))\n\ntext_data <- text_data %>% \n  filter(speaker != \"interviewer\")\n```\n\nPre-processing prepares your qualitative data for analysis by\n**cleaning**, **organizing**, and **standardizing** text. This step\nensures the data is usable for word frequency, sentiment analysis, topic\nmodeling, and other text mining techniques.\n\nYou should always assess the **scope and quality** of your data first:\n\n-   Are the data cleaned (lowercase, removed participant names,\n    punctuation, symbols)? \n\n```{r, echo=FALSE}\nomni::callout_box(\n    text = \"Check out our function that does all of these data cleaning tasks for you in R! You can also segment your text by question, combine multiple interviews and focus groups into one dataset, and more.\",\n    color = \"periwinkle-600\",\n    fixed_width_px = 800\n)\n```\n<center> \n  <a href=\"https://www.example.com\" target=\"_blank\" rel=\"noopener\" class=\"yellow-btn\">\n    Cleaning transcripts function\n  </a>\n</center>\n-   How much text do you have? (Word counts per document/response.)\n\n-   How many participants contributed?\n\n-   How detailed or shallow are the responses?\n\n-   Whose voices are you analyzing?\n\nOnce you understand your data, you can decide which pre-processing steps\nare appropriate.\n\n## Tokenization {.unnumbered}\n\nTokenization breaks text into individual pieces—usually words, but\nsometimes phrases or sentences.\n\n-   Most text mining techniques require tokenized text.\n\n-   This is common for word frequency analysis, sentiment analysis, and\n    topic modeling.\n\n-   After tokenization, we lose the flow of sentences which is generally\n    okay for some analyses, but not for narrative analysis or content\n    analysis.\n\n```{r}\n# %%%%%%%%%%%%%%% Tokenize text %%%%%%%%%%%%%%%\n\n# Tokenize text\ntokenized_text <- text_data %>% \n  unnest_tokens(word, text)\n\n```\n\n**What are Stop Words?**\n\nStop words are common words like \"the\", \"and\", \"but\" that don’t add much\nmeaning on their own. Because we want to focus on broader themes from\nour participants' voices, we usually want to remove stop words after\ntext is tokenized. This process allows us to focus on content-rich\nwords, like nouns and verbs. We should remove stop words when running a\nword frequency analysis, topic modeling, or sentiment analysis.\n\nWe can also create custom stop words that we want to remove from our\nanalyses, for example, location names or project-specific words that\nappear frequently but aren’t relevant to the analysis.\n\n**Example:**\n\n-   In the example below, my custom stop words are \"travis\" and\n    \"county\", and they're added to the stop_words bank which includes\n    other words like \"but\", \"is\", \"the\", etc. If you'd like to see all\n    of the stop words, see View(stop_words).\n\n```{r}\n# %%%%%%%%%%%%%%% Stop words %%%%%%%%%%%%%%%\n\n# Use standard stop words + custom ones (e.g., counties, names)\nmy_stop_words <- bind_rows(stop_words, \n                           data.frame(word = c(\"travis\", \n                                               \"county\"), lexicon = \"custom\"))\n\ncleaned_text <- tokenized_text %>% \n  anti_join(my_stop_words, by = \"word\")\n\n```\n\n## Stemming {.unnumbered}\n\nStemming words cuts them down to their root forms (e.g., \"running\"\nbecomes \"run\"). This pre-processing step should primarily be used for\ntopic modeling or for other analyses that you decide that exact word\nform isn’t critical. This step is recommended to use with larger\ndatasets, with typically hundreds of documents or more, but for our\npurposes we recommend stemming any time you have enough data for topic\nmodeling (around 5,000 total words).\n\n```{r}\n# %%%%%%%%%%%%%%% Stemming %%%%%%%%%%%%%%%\n\nlibrary(textstem)\n\nstemmed_text <- cleaned_text %>%\n  mutate(stem_word = stem_words(word))\n```\n\n## Lemmatization {.unnumbered}\n\nLemmatization converts words to their dictionary root and keeps the\nwords readable to the user (e.g., \"better\" becomes \"good\"). This\npre-processing step should be used for word frequency analysis,\nsentiment analysis, or thematic/content analysis as it works will with\nsmall and large corpuses.\n\n```{r}\n\n# %%%%%%%%%%%%%%% Lemmatization %%%%%%%%%%%%%%%\n\nlemmatized_text <- cleaned_text %>% \n  mutate(lem_word = lemmatize_words(word))\n\n```\n\nLet's compare the stem words, vs lemmatized words against the original\ntokenized words:\n\n```{r}\ncompare_words <- cleaned_text %>% \n  left_join(stemmed_text, \n            by = join_by(line, \n                         speaker, \n                         participant_id, question, section, section_label,\n                         session, word)) %>% \n  left_join(lemmatized_text,\n            by = join_by(line, \n                         speaker, \n                         participant_id, question, section, section_label,\n                         session, word)) %>%\n  slice(1:10) %>%\n  kable()\n```\n\n```{r, include=TRUE}\ncompare_words\n```\n\n```{r, echo=FALSE}\n# Create a data frame for the table\ncomparison_table <- data.frame(\n  \"Stemming\" = c(\n    \"You need speed over accuracy\",\n    \"Working with large datasets\",\n    \"Basic information retrieval tasks\",\n    \"Search engines or topic modeling where fine detail isn't critical\",\n    \"Not concerned about human readability\"\n  ),\n  \"Lemmatization\" = c(\n    \"Slower but more accurate\",\n    \"Better for small datasets\",\n    \"Better for nuanced analysis\",\n    \"Better for in-depth analysis\",\n    \"Maintains readable words)\"\n  )\n)\n\n# Print the table\nkable(comparison_table,\n      caption = \"When to Use Stemming vs. Lemmatization\",\n      align = 'll')\n\n```\n\n<br>\n\n```{=html}\n<div class=\"outer\">\n  <div class=\"green-box\"\n       style=\"--bar-bg: #F7b925;\n              --bar-text-color: #FFFFFF;\n              --box-h: 90px;\">\n    <div class=\"green-col\">Analyses</div>\n    <div class=\"green-col pattern\">\n      <img src=\"images/Pattern-06-Yellow.png\" alt=\"Pattern\">\n    </div>\n  </div>\n</div>\n```\n\n<br>\n\n## Word frequency analysis {.unnumbered}\n\n::::: columns\n::: {.column width=\"30%\"}\n![](images/analysis3.png)\n:::\n\n::: {.column width=\"70%\"}\nWord frequency analysis is a simple and effective method that counts how\noften specific words appear in your dataset. It’s often used early in\nqualitative analysis to get a general sense of prominent topics, but it\ncan also be applied as a stand-alone method to identify frequently\nmentioned issues in participant responses. Word frequency works best\nwhen you have a reasonable amount of text to analyze. As a general best\npractice, having at least 100–200 content words (excluding stop words)\nallows for more reliable interpretation of patterns, though smaller\ndatasets can still offer preliminary insights.\n\nTo ensure accuracy, it’s important to pre-process your text. Using\nlemmatized words helps avoid counting variations like “run” and\n“running” separately, and removing stop words—common words such as “the”\nor “and”—allows you to focus on terms that carry more meaning. In cases\nwhere participants mention names of places or projects repeatedly,\ncustom stop word lists can also help refine the results.\n:::\n:::::\n\nWhile word frequency counts themselves do not explain context or\nmeaning, they can help point to emerging themes by highlighting concepts\nthat recur across participants or within particular sections of\ndiscussion. For example, if words like “transportation,” “access,” and\n“barrier” frequently appear in responses about service challenges, they\nsignal a potential theme that warrants deeper exploration. Word\nfrequency analysis can also help guide more interpretive methods such as\nthematic or content analysis, and is most useful when findings are\ncontextualized within the broader dataset and research goals.\n\n```{r}\n# %%%%%%%%%%%%%%% Word frequencies %%%%%%%%%%%%%%%\n# Word counts\nword_counts <- lemmatized_text %>%\n  count(word, sort = TRUE)\n\nhead(word_counts)\n\n# Visualize the word counts\n\nword_counts %>%\n  top_n(5) %>%\n  ggplot(aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Top 10 Most Frequent Words\", x = \"Word\", y = \"Count\") +\n  theme_omni()\n\n```\n\n## Sentiment analysis {.unnumbered}\n\n**Sentiment analysis** is a method that measures the emotional tone of\ntext by categorizing words or phrases as positive, negative, or neutral.\nIt can be used to quickly gauge participants’ attitudes toward a topic\nor to assess the overall tone of responses across interviews, focus\ngroups, or surveys. Sentiment analysis works best when you have larger\namounts of text—ideally, datasets with several hundred words or\nmore—because emotional tone can vary within short responses, making\nsentiment harder to interpret in very small datasets. However, it can\nstill offer insights in smaller datasets when applied carefully and when\nresults are presented as exploratory.\n\nBest practices for sentiment analysis include pre-processing your text\nby removing stop words and standardizing language through lemmatization.\nThis ensures consistent scoring and avoids misclassifications due to\nword variations. Sentiment analysis typically relies on pre-built\nlexicons, such as Bing, AFINN, or the NRC Emotion Lexicon, which assign\nemotional values to words. It’s important to note that these lexicons\nwere often designed for general use (such as social media text) and may\nrequire customization to fit specific public health or social science\ncontexts. For example, in health-related interviews, a word like\n\"treatment\" might appear frequently and carry different sentiment\ndepending on the discussion’s focus.\n\nWhile sentiment analysis doesn’t capture nuance or context in the way\nmanual coding can, it can highlight patterns of emotional tone across\ndatasets or within specific discussion topics. For example, sentiment\nanalysis might reveal that participants express more negative sentiment\nwhen discussing barriers to accessing services, and more positive\nsentiment when discussing recommendations for future improvements. These\ninsights can help guide deeper qualitative coding or serve as an\nadditional layer of analysis to support findings. As with any automated\nmethod, it’s important to review and interpret results in context and to\ndocument any limitations or adaptations made to the analysis.\n\n```{r}\n\nlibrary(tidytext)\n\n# Get sentiment lexicon (if you wanted to test for positive, negative, and 8 emotions, replace \"bing\" with \"nrc\")\nbing <- get_sentiments(\"bing\")\n\n# Join with your text\nsentiment_data <- lemmatized_text %>%\n  inner_join(bing, by = \"word\")\n\n# Count positive vs negative\nsentiment_summary <- sentiment_data %>%\n  count(sentiment)\n\nsentiment_summary\n\n# Visualize\nsentiment_summary %>%\n  ggplot(aes(x = sentiment, y = n, fill = sentiment)) +\n  geom_col() +\n  labs(title = \"Sentiment Analysis\", x = \"Sentiment\", y = \"Word Count\") +\n  scale_fill_omni_discrete() +\n  theme_omni()\n\n```\n\n```{r}\n\n# Filter for the \"Recommendations\" section\nrec_data <- lemmatized_text %>%\n  filter(section_label == \"Recommendations\")\n\n# Join with the Bing sentiment lexicon\nbing_sentiment <- get_sentiments(\"bing\")\n\nrec_data <- rec_data %>%\n  inner_join(bing_sentiment, by = \"word\")\n\n# Calculate sentiment score by line\nsentiment_scores <- rec_data %>%\n  count(line, sentiment) %>%\n  pivot_wider(\n    names_from = sentiment,\n    values_from = n,\n    values_fill = list(n = 0)   # Fill missing sentiment types with 0\n  ) %>%\n  mutate(\n    positive = if_else(is.na(positive), 0L, positive),\n    negative = if_else(is.na(negative), 0L, negative),\n    sentiment_score = positive - negative\n  )\n\n# Join sentiment scores back to the original text\nrec_data_with_scores <- cleaned_text %>%\n  left_join(sentiment_scores, by = \"line\")\n\n```\n\n**Tip for finding quotes by tone**\n\nLet's say you want to pull a quote to include in a report and you know\nthat you want it to be a positively toned quote in a \"recommendations\"\nsection of your discussion. You can use the sentimentr package to gather\nsentiments of each sentence (or segment) and sort by sentiment score to\nfind the quote with the most positive tone in your criteria.\n\nSee an example below:\n\n```{r}\n\nlibrary(sentimentr)\n\ntext_data_wsentiment <- sentiment(text_data$text)\n\nsentiment_summary <- text_data_wsentiment %>%\n  group_by(element_id) %>%\n  summarize(\n    avg_sentiment = mean(sentiment, na.rm = TRUE),\n    sd_sentiment = sd(sentiment, na.rm = TRUE),\n    n_sentences = n()\n  )\n\n# Combine summarized sentiment scores with your original data\ntext_data_with_sentiment <- text_data %>%\n  mutate(element_id = row_number()) %>%\n  left_join(sentiment_summary, by = \"element_id\")\n\n# View the combined data\ntext_data_with_sentiment\n```\n\n**Mixed-methods with sentiment analysis**\n\nBecause sentiment analysis gives you a numeric output as a sentiment\nscore, you can imagine instances where you may want to compare sentiment\nscores between groups, correlate sentiment scores with other variables\nin a quantitative survey, or conduct pre-post comparisons.\n\n## Topic modeling {.unnumbered}\n\nTopic modeling is an automated method used to identify themes or topics\nacross large collections of text. It uses algorithms to group together\nwords that frequently appear in similar contexts, helping reveal hidden\npatterns or structures in qualitative data. Topic modeling is especially\nuseful when you have large datasets—typically a minimum of 5,000 words\nor more spread across multiple documents or participant responses. The\nmethod works best when documents are of relatively similar length, which\nhelps the model assign topics more evenly.\n\nThis method is **not** recommended for use on smaller corpuses and would\nbe considered poor practice to rely on it to generate meaningful\ninsights. If you don't have enough data for this analysis, consider\nusing the content analysis strategy to identify themes in the data.\n\nBefore running a topic model, it’s important to pre-process your text.\nBest practices include stemming words to reduce variation (so that “run”\nand “running” are treated as the same word) and removing stop words to\nfocus on meaningful content. Topic modeling doesn’t require predefined\ncodes or themes, making it a good exploratory tool for surfacing\nunexpected topics in your data. However, it’s important to remember that\nthese topics are generated algorithmically—they group terms based on\nstatistical patterns, not human interpretation. As a result, human\nreview is always needed to interpret and label the topics in a way that\nmakes sense for your project and participants.\n\nTopic modeling can complement manual coding by offering a high-level\nview of common themes, pointing analysts toward areas that may warrant\ndeeper exploration. For example, a topic model might surface clusters of\nwords related to barriers (“transportation,” “access,” “cost”) and\nanother cluster about solutions (“education,” “outreach,” “support”),\ngiving you a starting point for thematic analysis. Commonly used R\npackages for topic modeling include topicmodels, which provides\nalgorithms like Latent Dirichlet Allocation (LDA), and tm for\npre-processing and managing textual data.\n\n```{r}\n\nlibrary(tm)\nlibrary(topicmodels)\n\n# Create Document-Term Matrix (DTM)\ndtm <- stemmed_text %>%\n  count(document = 1, word) %>%\n  cast_dtm(document, word, n)\n\n# Fit LDA models with 2 and 3 topics\nlda_model_2 <- LDA(dtm, k = 2, control = list(seed = 1234))\nlda_model_3 <- LDA(dtm, k = 3, control = list(seed = 1234))\n\n# Compare perplexity (lower is better)\nperplexity_2 <- perplexity(lda_model_2)\nperplexity_3 <- perplexity(lda_model_3)\n\n# Print perplexity values to see how many topics we should go with- in this case 2 topics perform better than 3\nperplexity_2\nperplexity_3\n\n# View top terms for both models\ntopics_2 <- tidy(lda_model_2, matrix = \"beta\")\ntopics_3 <- tidy(lda_model_3, matrix = \"beta\")\n\n# Top terms for 2-topic model\ntop_terms_2 <- topics_2 %>%\n  group_by(topic) %>%\n  top_n(10, beta) %>%\n  arrange(topic, -beta)\n\n# Top terms for 3-topic model\ntop_terms_3 <- topics_3 %>%\n  group_by(topic) %>%\n  top_n(10, beta) %>%\n  arrange(topic, -beta)\n\n# Display the top terms to compare interpretability\ntop_terms_2\ntop_terms_3\n\n```\n\n## Content analysis {.unnumbered}\n\nContent analysis is a method used to count how often predefined\nconcepts, themes, or categories appear in qualitative data. It relies on\na dictionary—a list of key terms or phrases—designed to reflect your\nevaluation questions or coding framework. Content analysis works well\nfor both small and large datasets, making it a flexible tool when you\nwant to systematically measure the presence of specific themes across\ninterviews, focus groups, or open-ended survey responses.\n\nA key requirement for effective content analysis is a carefully designed\ndictionary that accurately captures the concepts you're interested in.\nThis might include terms related to barriers, facilitators, or\nrecommendations, depending on the project’s goals. Best practice is to\nvalidate the dictionary by reviewing examples of matched text to make\nsure the terms are identifying the intended content. You may need to\nrefine the dictionary over time, adding synonyms or removing words that\ngenerate false positives.\n\nContent analysis can help answer questions like \"How frequently do\nparticipants mention prevention strategies?\" or \"What percentage of\nresponses reference funding challenges?\" It is particularly useful when\nyou need to quantify qualitative data for reporting purposes, or when\nyou want to compare how frequently themes appear across different\nstakeholder groups. In R, the quanteda package offers efficient tools\nfor dictionary-based content analysis, allowing you to apply a\ndictionary and quickly summarize how often key terms or concepts appear\nin the dataset.\n\n```{r}\n\nlibrary(quanteda)\n\n# Create the corpus from your text column\ncorp <- corpus(text_data$text)\n\n# Create tokens from the corpus\ntokens_obj <- quanteda::tokens(corp, \n                     what = \"word\", \n                     remove_punct = TRUE, \n                     remove_symbols = TRUE)\n\n# Create the document-feature matrix (dfm) from tokens\ndfm_obj <- dfm(tokens_obj, \n               tolower = TRUE)\n\n# View your dfm\ndfm_obj\n\n# Define dictionary and add themes with keywords here\ndict <- dictionary(list(\n\n  prevention = c(\n    \"prevention\", \"educate\", \"education\", \"awareness\", \"outreach\",\n    \"campaign\", \"community education\", \"school programs\", \"early intervention\",\n    \"public awareness\", \"information\", \"training\", \"workshops\", \n    \"parent education\", \"youth programs\", \"risk reduction\", \"media campaigns\"\n  ),\n  \n  treatment = c(\n    \"treatment\", \"therapy\", \"rehab\", \"rehabilitation\", \"detox\", \"detoxification\",\n    \"MAT\", \"medication-assisted treatment\", \"buprenorphine\", \"methadone\",\n    \"naltrexone\", \"suboxone\", \"clinics\", \"recovery support\", \"residential programs\",\n    \"counseling\", \"peer support\", \"case management\", \"behavioral therapy\",\n    \"inpatient\", \"outpatient\", \"continuum of care\", \"access to care\"\n  ),\n  \n  harm_reduction = c(\n    \"harm reduction\", \"naloxone\", \"narcan\", \"needle exchange\", \"syringe service\",\n    \"safe injection\", \"supervised consumption\", \"overdose prevention\", \n    \"fentanyl test strips\", \"distribution\", \"drug checking\", \"safe supply\", \n    \"wound care\", \"community outreach\", \"safer use education\", \"condom distribution\",\n    \"reduction of risk\", \"public health response\"\n  )\n))\n\n\n# Apply dictionary\ndfm_dict <- dfm_lookup(dfm_obj, dictionary = dict)\n\ndfm_dict \n\n# Convert dictionary to dataframe to join with original data\ndfm_df <- convert(dfm_dict, to = \"data.frame\")\n\n# Add row ID to original text data so we can join on it\ntext_data2 <- text_data %>%\n  mutate(doc_id = paste0(\"text\", row_number()))\n\n# Make sure the doc_id matches what's in the DFM\nhead(dfm_df$doc_id)\n\n# Join the DFM (now a data frame) back to your text data by doc_id\ntext_with_counts <- text_data2 %>%\n  left_join(dfm_df, by = \"doc_id\")\n\n# View the combined data\nhead(text_with_counts)\n\n```\n\n## Thematic Analysis in Dedoose\n\n@qualBPT help here!! Can we add anything re coding to this guide in this section, maybe Ivonne's work? What other programs should we add?\n\nIf using Dedoose to conduct thematic analysis, we recommend a\nstructured, reflective approach grounded in reflexive thematic analysis\n(Clarke & Braun, 2006) and informed by critical realism.\n\nHere’s how to approach it:\n\n1.  Start with Familiarization Read through transcripts or responses in\n    full before coding.\n\nUse memos to reflect on initial impressions, patterns, and researcher\nassumptions.\n\n2.  Develop Codes Iteratively Begin with inductive coding (bottom-up),\n    focusing on what participants actually say.\n\nAvoid pre-loading the codebook unless doing deductive analysis is needed\nfor evaluation purposes.\n\nUse Dedoose’s “Descriptor” fields to track context (e.g., stakeholder\ngroup, session type) for later analysis.\n\n3.  Apply Codes Reflexively Apply codes carefully, updating definitions\n    and merging/splitting codes as needed.\n\nEncourage coders to discuss disagreements and maintain an audit trail of\nkey coding decisions.\n\n4.  Construct Themes Thoughtfully After coding, review excerpts by code\n    and group them into broader themes that reflect patterns across\n    participants.\n\nUse Dedoose’s “Code Co-Occurrence” and “Code Application” tools to\nexplore theme structure.\n\n5.  Acknowledge Researcher Role Reflect on how your perspective,\n    language, and interpretation influence theme development.\n\nBe cautious not to overstate findings; describe patterns and nuance\nrather than asserting consensus.\n\n6.  Document Everything Keep a record of how codes and themes evolved.\n\nClearly state the number of participants or excerpts that informed each\ntheme.\n\nMake your interpretive lens and limitations explicit—especially when\ndatasets are small or uneven across groups.\n\n> Thematic analysis in Dedoose should still reflect Omni’s commitment to\n> critical realism: treat participant input as reflecting real issues\n> shaped by context and perspective, and be transparent about what the\n> data can—and cannot—support.\n\n## Combining analyses {.unnumbered}\n\nNo single method can fully capture the richness and complexity of\nqualitative data. At Omni, we often combine different qualitative\nanalysis approaches to explore various angles of our data and answer\nnuanced evaluation questions. Pairing methods like word frequency,\nsentiment analysis, content analysis, and thematic analysis can help\nreveal both patterns and meaning, ensuring our findings are grounded in\nevidence and provide actionable insights.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":{"omni::html_report":{"toc":true,"toc_float":true,"main_font":"Inter Tight","background_color":"#f9f7f4","remove_logo":false}},"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"qual_analysis__R_guide.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","title":"Qualitative Analysis Guide","subtitle":"","author":"Hannah","date":"`r Sys.Date() |> format('%B %d, %Y')`"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}